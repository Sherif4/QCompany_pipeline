{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0a705e-d013-4349-b873-78fcb1d1cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    " import re\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, regexp_replace, trim,when ,monotonically_increasing_id,lit,year, month, dayofmonth, weekofyear, dayofweek, date_format,floor,dense_rank,\\\n",
    "substring,concat,split, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
    "from datetime import date, datetime, timedelta\n",
    "import subprocess\n",
    "from py4j.java_gateway import java_import\n",
    "import os\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbd3353-908f-4d90-850f-c36424d80817",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .appName(\"sales_transactions\")\\\n",
    "    .config(\"spark.eventLog.logBlockUpdates.enabled\", True)\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6120cae5-dc75-4881-b2c2-139445c22d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240709 17\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "date_str = now.strftime(\"%Y%m%d\")\n",
    "hour_str = now.strftime(\"%H\")\n",
    "print(date_str, hour_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf8fd60-448d-4761-9e0c-c69f3851b8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-----------+--------------+--------------+--------------+---------+----------+---------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+--------------------+-------------+\n",
      "|transaction_date|  transaction_id|customer_id|customer_fname|customer_lname|sales_agent_id|branch_id|product_id|   product_name|product_category|offer_1|offer_2|offer_3|offer_4|offer_5|units|unit_price|is_online|payment_method|shipping_address|      customer_email|discount_perc|\n",
      "+----------------+----------------+-----------+--------------+--------------+--------------+---------+----------+---------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+--------------------+-------------+\n",
      "|       2023-5-20|trx-152546429674|      85469|     Alexander|         Brown|           1.0|      2.0|        22|   Coffee Maker|      Appliances|   null|   null|   null|   null|   null|   10|     79.99|       no|          Cash|            null|alexander.brown@g...|            0|\n",
      "|      2022-10-25|trx-291375327542|      85512|       William|         Brown|           3.0|      1.0|        24|        Blender|      Appliances|   null|   null|   null|   true|   null|    5|     49.99|       no|          Cash|            null|william.brown@gma...|           20|\n",
      "|        2022-2-5|trx-312507679871|      85484|          John|      Williams|          10.0|      3.0|         4|     Headphones|     Electronics|   null|   null|   null|   null|   null|    1|     99.99|       no|   Credit Card|            null|john.williams@gma...|            0|\n",
      "|      2023-10-20|trx-193384855491|      85528|     Alexander|        Miller|           7.0|      2.0|        25|Washing Machine|      Appliances|   null|   null|   null|   null|   null|    8|    499.99|       no|          Cash|            null|alexander.miller@...|            0|\n",
      "|      2022-11-17|trx-831626097654|      85500|          John|         Brown|           5.0|      1.0|        14|         Camera|     Electronics|   null|   null|   true|   null|   null|   10|    399.99|       no|          Cash|            null|john.brown@hotmai...|           15|\n",
      "+----------------+----------------+-----------+--------------+--------------+--------------+---------+----------+---------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df = spark.read.parquet(f\"hdfs:///data/retail_silver/{date_str}/{hour_str}/sales_transactions_SS_cleaned_{date_str}_{hour_str}.parquet\")\n",
    "input_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b91cd6-af65-4ec1-becf-74331a468d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_fname: string (nullable = true)\n",
      " |-- customer_lname: string (nullable = true)\n",
      " |-- sales_agent_id: double (nullable = true)\n",
      " |-- branch_id: double (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- offer_1: boolean (nullable = true)\n",
      " |-- offer_2: boolean (nullable = true)\n",
      " |-- offer_3: boolean (nullable = true)\n",
      " |-- offer_4: boolean (nullable = true)\n",
      " |-- offer_5: boolean (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- is_online: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- shipping_address: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- discount_perc: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d197f3-24a9-47f2-9bc5-566b12afe3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function_to_rename_in_hdfs\n",
    "def rename_in_hdfs(golden_layer_path,file_extension,name):\n",
    "    # Run the Hadoop fs -ls command to list files\n",
    "    list_files_process = subprocess.run([\"hadoop\", \"fs\", \"-ls\", golden_layer_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Check for errors\n",
    "    if list_files_process.returncode != 0:\n",
    "        print(f\"Error listing files in {golden_layer_path}: {list_files_process.stderr.decode()}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Decode stdout to string format and split lines\n",
    "    stdout_str = list_files_process.stdout.decode()\n",
    "    file_list = stdout_str.splitlines()\n",
    "\n",
    "    # Find the file to rename based on criteria\n",
    "    file_to_rename = None\n",
    "    for line in file_list:\n",
    "        if line.endswith(file_extension):\n",
    "            file_to_rename = line.split()[-1].strip()\n",
    "            break\n",
    "\n",
    "    # Check if a file matching the criteria was found\n",
    "    if file_to_rename:\n",
    "        new_filename = f\"{golden_layer_path}/{name}{file_extension}\"\n",
    "\n",
    "        # Move (rename) the file\n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-mv\", file_to_rename, new_filename])\n",
    "\n",
    "        print(f\"File moved and renamed to: {new_filename}\")\n",
    "    else:\n",
    "        print(\"File matching the criteria not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38c7b45-96e0-4758-ac81-ae7461673030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a file exists in HDFS\n",
    "def check_if_exists(path):\n",
    "    jvm = spark._jvm\n",
    "    jsc = spark._jsc\n",
    "    fs = jvm.org.apache.hadoop.fs.FileSystem.get(jsc.hadoopConfiguration())\n",
    "    return fs.exists(jvm.org.apache.hadoop.fs.Path(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfa2dd-6f83-42e4-8a1a-ef890946e254",
   "metadata": {},
   "source": [
    "A function that writes a df to hive external_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06701e99-1036-47c3-a789-455f56bc216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df_to_table(database_name, table_name, df, partition_columns=None, bucket_column=None, num_buckets=None):\n",
    "    # Temporary table name\n",
    "    temp_table = \"temp_table\"\n",
    "\n",
    "    # Create DataFrameWriter with overwrite mode\n",
    "    writer = df.write.mode(\"overwrite\")\n",
    "\n",
    "    # If partition columns are provided, specify them in the writer\n",
    "    if partition_columns:\n",
    "        if isinstance(partition_columns, list):\n",
    "            writer = writer.partitionBy(*partition_columns)\n",
    "        else:\n",
    "            writer = writer.partitionBy(partition_columns)\n",
    "\n",
    "    # If bucket column and number of buckets are provided, specify them in the writer\n",
    "    if bucket_column and num_buckets:\n",
    "        writer = writer.bucketBy(num_buckets, bucket_column)\n",
    "\n",
    "    # Save the DataFrame to a temporary table\n",
    "    writer.saveAsTable(temp_table)\n",
    "\n",
    "    # Construct the insert overwrite query\n",
    "    if partition_columns:\n",
    "        if isinstance(partition_columns, list):\n",
    "            partition_str = \", \".join(partition_columns)\n",
    "        else:\n",
    "            partition_str = partition_columns\n",
    "    else:\n",
    "        partition_str = \"\"\n",
    "\n",
    "    insert_query = f\"\"\"\n",
    "        INSERT OVERWRITE TABLE {database_name}.{table_name}\n",
    "        {f'PARTITION ({partition_str})' if partition_str else ''}\n",
    "        SELECT * FROM {temp_table}\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the insert overwrite query\n",
    "    spark.sql(insert_query)\n",
    "    \n",
    "    # Drop the temporary table after use\n",
    "    spark.sql(f\"DROP TABLE {temp_table}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bba9bfe8-67ad-4dcc-afdd-79f262fac181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------------+--------------+--------------------+\n",
      "|customer_sur_key|customer_id|customer_fname|customer_lname|      customer_email|\n",
      "+----------------+-----------+--------------+--------------+--------------------+\n",
      "|               1|      85462|          John|         Smith|john.smith@yahoo.com|\n",
      "|               2|      85463|        Olivia|         Smith|olivia.smith@outl...|\n",
      "|               3|      85464|       Michael|        Miller|michael.miller@ou...|\n",
      "|               4|      85465|        Sophia|        Miller|sophia.miller@out...|\n",
      "|               5|      85466|          Emma|         Brown|emma.brown@yahoo.com|\n",
      "+----------------+-----------+--------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "File moved and renamed to: hdfs:///data/retail_gold/customer_dim/customer_dim.parquet\n"
     ]
    }
   ],
   "source": [
    "#write customer dim in HDFS\n",
    "cust_data = input_df.select('customer_id', 'customer_fname', 'customer_lname', 'customer_email')\n",
    "cust_data = cust_data.dropDuplicates(['customer_id'])\n",
    "\n",
    "golden_layer_path=\"hdfs:///data/retail_gold/customer_dim\"\n",
    "file_extension = \".parquet\"\n",
    "name='customer_dim'\n",
    "path_to_check = f\"hdfs:///data/retail_gold/customer_dim/{name}{file_extension}\"\n",
    "\n",
    "if check_if_exists(golden_layer_path):\n",
    "    cust_dim = spark.read.parquet(\"/data/retail_gold/customer_dim/customer_dim.parquet\")\n",
    "    existing_cust_dim_without_sk = cust_dim.select('customer_id', 'customer_fname', 'cusomter_lname', 'customer_email')\n",
    "    new_customers_data = cust_data.subtract(existing_cust_dim_without_sk)\n",
    "    print(new_customers_data)\n",
    "    \n",
    "    # Get the maximum surrogate key from existing data\n",
    "    max_sur_key = cust_dim.agg({\"customer_sur_key\": \"max\"}).collect()[0][0]\n",
    "    print(max_sur_key)\n",
    "    \n",
    "    \n",
    "    # Combine existing data with new data\n",
    "    if new_customers_data.rdd.isEmpty() == False:\n",
    "        window_spec = Window.orderBy(\"customer_id\")\n",
    "       # Add surrogate keys to new data starting from max_sur_key + 1\n",
    "        customers_dim = new_customers_data.withColumn('customer_sur_key', (row_number().over(window_spec) + max_sur_key).cast(\"int\"))\n",
    "        customers_dim_sk = customers_dim.select('customer_sur_key','customer_id', 'customer_fname', 'customer_lname', 'customer_email')\n",
    "        \n",
    "        updated_customers_dim = cust_dim.union(customers_dim_sk)\n",
    "\n",
    "        updated_customers_dim = updated_customers_dim.repartition(1)\n",
    "            \n",
    "        updated_customers_dim.write.mode('overwrite') \\\n",
    "                            .option(\"header\", \"true\") \\\n",
    "                            .format('parquet') \\\n",
    "                            .save(f\"{golden_layer_path}/tmp\")\n",
    "\n",
    "        rename_in_hdfs(f\"{golden_layer_path}/tmp\", file_extension, name) \n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-rm\", path_to_check])\n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-mv\", f\"{golden_layer_path}/tmp/{name}{file_extension}\" ,golden_layer_path])\n",
    "        #write_df_to_table('retail_DWH', 'Customer_Dim', updated_customers_dim)\n",
    "        print(\"done\")\n",
    "    else:\n",
    "        updated_customers_dim = cust_dim\n",
    "        print(\"No new Data\")\n",
    "else:\n",
    "    # Add a sequential surrogate key column\n",
    "    window_spec = Window.orderBy(\"customer_id\")\n",
    "    updated_customers_dim = cust_data.withColumn('customer_sur_key', row_number().over(window_spec))\n",
    "\n",
    "    #to write cust_dim in one file \n",
    "    updated_customers_dim = updated_customers_dim.repartition(1)\n",
    "\n",
    "    #make customer dim \n",
    "    updated_customers_dim = updated_customers_dim.select('customer_sur_key','customer_id', 'customer_fname', 'customer_lname', 'customer_email') \n",
    "    updated_customers_dim.write.mode('overwrite') \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .format('parquet') \\\n",
    "            .save(golden_layer_path)\n",
    "    updated_customers_dim.show(5)\n",
    "    #write_df_to_table('retail_DWH', 'Customer_Dim', updated_customers_dim)\n",
    "\n",
    "    rename_in_hdfs(golden_layer_path,file_extension,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff04be8c-f132-4f0a-99f4-0474f7806761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+\n",
      "|product_id|product_name|product_category|\n",
      "+----------+------------+----------------+\n",
      "+----------+------------+----------------+\n",
      "\n",
      "30\n",
      "No new Data\n"
     ]
    }
   ],
   "source": [
    "# Ensure product_dim is distinct by product_id and add a sequential surrogate key\n",
    "# Select relevant columns for product_dim\n",
    "product_dim = input_df.select('product_id', 'product_name', 'product_category')\n",
    "\n",
    "# Drop duplicates based on product_id if necessary\n",
    "product_dim = product_dim.dropDuplicates(['product_id'])\n",
    "\n",
    "# Define the golden layer path and file details\n",
    "golden_layer_path = \"hdfs:///data/retail_gold/product_dim\"\n",
    "file_extension = \".parquet\"\n",
    "name = \"product_dim\"\n",
    "path_to_check = f\"hdfs:///data/retail_gold/product_dim/{name}{file_extension}\"\n",
    "\n",
    "if check_if_exists(path_to_check):\n",
    "    existing_product_dim = spark.read.parquet(path_to_check)\n",
    "    existing_product_dim = existing_product_dim.withColumn(\"product_sur_key\", col(\"product_sur_key\").cast(\"int\"))\n",
    "    existing_product_dim_without_sk = existing_product_dim.select('product_id', 'product_name', 'product_category')\n",
    "    \n",
    "    new_products_data = product_dim.subtract(existing_product_dim_without_sk)\n",
    "    print(new_products_data)\n",
    "    \n",
    "    # Get the maximum surrogate key from existing data\n",
    "    max_sur_key = existing_product_dim.agg({\"product_sur_key\": \"max\"}).collect()[0][0]\n",
    "    print(max_sur_key)\n",
    "    \n",
    "    # Combine existing data with new data\n",
    "    if new_products_data.rdd.isEmpty() == False:\n",
    "        window_spec = Window.orderBy(\"product_id\")\n",
    "        product_dim = new_products_data.withColumn('product_sur_key', (row_number().over(window_spec) + max_sur_key).cast(\"int\"))\n",
    "        product_dim_sk = product_dim.select('product_sur_key','product_id', 'product_name', 'product_category')\n",
    "        updated_product_dim = existing_product_dim.union(product_dim_sk)\n",
    "        updated_product_dim = updated_product_dim.repartition(1)\n",
    "        updated_product_dim.write.mode('overwrite') \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .format('parquet') \\\n",
    "                    .save(f\"{golden_layer_path}/tmp\")\n",
    "        \n",
    "        rename_in_hdfs(f\"{golden_layer_path}/tmp\", file_extension, name) \n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-rm\", path_to_check])\n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-mv\", f\"{golden_layer_path}/tmp/{name}{file_extension}\" ,golden_layer_path])\n",
    "    else:\n",
    "        updated_product_dim = existing_product_dim\n",
    "        print(\"No new Data\")\n",
    "else:\n",
    "    # Add a sequential surrogate key column\n",
    "    window_spec = Window.orderBy(\"product_id\")\n",
    "    product_dim = product_dim.withColumn('product_sur_key', row_number().over(window_spec))\n",
    "    updated_product_dim = product_dim\n",
    "\n",
    "    updated_product_dim = updated_product_dim.select('product_sur_key','product_id', 'product_name', 'product_category')\n",
    "\n",
    "    updated_product_dim.show()\n",
    "    \n",
    "    # Repartition to one file for efficient writing\n",
    "    product_dim = product_dim.repartition(1)\n",
    "\n",
    "    # Write the updated data back to HDFS\n",
    "    updated_product_dim.write.mode('overwrite') \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .format('parquet') \\\n",
    "        .save(golden_layer_path)\n",
    "\n",
    "    # Rename the file in HDFS if necessary\n",
    "    rename_in_hdfs(golden_layer_path, file_extension,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f04e839-8863-448a-90e5-13e1ff09c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------+-----+\n",
      "|branch_id|   location|establish_date|class|\n",
      "+---------+-----------+--------------+-----+\n",
      "|        1|   New York|    2017-01-15|    A|\n",
      "|        2|Los Angeles|    2016-07-28|    B|\n",
      "|        3|    Chicago|    2015-03-10|    A|\n",
      "|        4|    Houston|    2016-11-05|    D|\n",
      "|        5|    Phoenix|    2017-09-20|    C|\n",
      "+---------+-----------+--------------+-----+\n",
      "\n",
      "+---------+--------+--------------+-----+\n",
      "|branch_id|location|establish_date|class|\n",
      "+---------+--------+--------------+-----+\n",
      "+---------+--------+--------------+-----+\n",
      "\n",
      "5\n",
      "No new Data\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for the initial CSV data and the golden layer path on HDFS\n",
    "file_path = f\"hdfs:///data/retail_bronze/{date_str}/{hour_str}/branches_SS_raw_{date_str}_{hour_str}.parquet\"\n",
    "golden_layer_path = \"hdfs:///data/retail_gold/branches_dim\"\n",
    "file_extension = \".parquet\"\n",
    "name = \"branches_dim\"\n",
    "path_to_check = f\"hdfs:///data/retail_gold/branches_dim/{name}{file_extension}\"\n",
    "\n",
    "# Load the CSV data into a PySpark DataFrame\n",
    "branches_dim = spark.read.parquet(file_path)\n",
    "\n",
    "# Convert establish_date to date type if needed\n",
    "branches_dim = branches_dim.withColumn(\"establish_date\", col(\"establish_date\").cast(\"date\"))\n",
    "print(branches_dim)\n",
    "\n",
    "# Drop duplicates based on branch_id if necessary\n",
    "branches_dim = branches_dim.dropDuplicates(['branch_id'])\n",
    "\n",
    "if check_if_exists(path_to_check):\n",
    "    existing_branch_dim = spark.read.parquet(path_to_check)\n",
    "    existing_branch_dim_without_sk = existing_branch_dim.select('branch_id', 'location', 'establish_date', 'class')\n",
    "    \n",
    "    new_branches_data = branches_dim.subtract(existing_branch_dim_without_sk)\n",
    "    print(new_branches_data)\n",
    "    \n",
    "    # Get the maximum surrogate key from existing data\n",
    "    max_sur_key = existing_branch_dim.agg({\"branch_sur_key\": \"max\"}).collect()[0][0]\n",
    "    print(max_sur_key)\n",
    "    # Add surrogate keys to new data starting from max_sur_key + 1\n",
    "\n",
    "    \n",
    "    # Combine existing data with new data\n",
    "    \n",
    "    if new_branches_data.rdd.isEmpty() == False:\n",
    "        window_spec = Window.orderBy(\"branch_id\")\n",
    "        branches_dim = new_branches_data.withColumn('branch_sur_key', (row_number().over(window_spec) + max_sur_key).cast(\"int\"))\n",
    "        branches_dim_sk = branches_dim.select('branch_sur_key','branch_id', 'location', 'establish_date', 'class')\n",
    "        \n",
    "        updated_branches_dim = existing_branch_dim.union(branches_dim_sk)\n",
    "        updated_branches_dim = updated_branches_dim.repartition(1)\n",
    "        updated_branches_dim.write.mode('overwrite') \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .format('parquet') \\\n",
    "                    .save(f\"{golden_layer_path}/tmp\")\n",
    "        \n",
    "        rename_in_hdfs(f\"{golden_layer_path}/tmp\", file_extension, name) \n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-rm\", path_to_check])\n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-mv\", f\"{golden_layer_path}/tmp/{name}{file_extension}\" ,golden_layer_path])\n",
    "    else:\n",
    "        updated_branches_dim = existing_branch_dim\n",
    "        print(\"No new Data\")\n",
    "else:\n",
    "    # Add a sequential surrogate key column\n",
    "    window_spec = Window.orderBy(\"branch_id\")\n",
    "    branches_dim = branches_dim.withColumn('branch_sur_key', row_number().over(window_spec))\n",
    "    updated_branches_dim = branches_dim\n",
    "\n",
    "    updated_branches_dim = updated_branches_dim.select('branch_sur_key', 'branch_id', 'location', 'establish_date', 'class')\n",
    "\n",
    "    updated_branches_dim.show()\n",
    "\n",
    "    # Write the updated data back to HDFS\n",
    "    updated_branches_dim.write.mode('overwrite') \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .format('parquet') \\\n",
    "        .save(golden_layer_path)\n",
    "\n",
    "    rename_in_hdfs(golden_layer_path, file_extension, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0c9bafc-3338-4fd9-bc8f-1797874a62bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+------------------+----------+\n",
      "|sales_agent_sur_key|sales_person_id|              name| hire_date|\n",
      "+-------------------+---------------+------------------+----------+\n",
      "|                  1|              1|          John Doe|2020-06-03|\n",
      "|                  2|              2|        Jane Smith|2018-05-13|\n",
      "|                  3|              3|   Michael Johnson|2021-10-03|\n",
      "|                  4|              4|       Emily Brown|2020-10-25|\n",
      "|                  5|              5|      David Wilson|2021-04-08|\n",
      "|                  6|              6|       Emma Taylor|2019-03-28|\n",
      "|                  7|              7|Christopher Miller|2020-01-11|\n",
      "|                  8|              8|      Olivia Davis|2021-10-24|\n",
      "|                  9|              9|   Daniel Martinez|2018-10-08|\n",
      "|                 10|             10|      Sophia Moore|2019-05-25|\n",
      "+-------------------+---------------+------------------+----------+\n",
      "\n",
      "10\n",
      "No new Data\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for the initial CSV data and the golden layer path on HDFS\n",
    "file_path = f\"hdfs:////data/retail_bronze/{date_str}/{hour_str}/sales_agents_SS_raw_{date_str}_{hour_str}.parquet\"\n",
    "golden_layer_path = \"hdfs:///data/retail_gold/sales_agent_dim\"\n",
    "file_extension = \".parquet\"\n",
    "name = \"sales_agent_dim\"\n",
    "path_to_check = f\"hdfs:///data/retail_gold/sales_agent_dim/{name}{file_extension}\"\n",
    "\n",
    "# Load the CSV data into a PySpark DataFrame\n",
    "agent_dim = spark.read.parquet(file_path)\n",
    "\n",
    "# Convert hire_date to date type if needed\n",
    "agent_dim = agent_dim.withColumn(\"hire_date\", col(\"hire_date\").cast(\"date\"))\n",
    "\n",
    "# Drop duplicates based on sales_person_id if necessary\n",
    "agent_dim = agent_dim.dropDuplicates(['sales_person_id'])\n",
    "\n",
    "if check_if_exists(path_to_check):\n",
    "    existing_agent_dim = spark.read.parquet(path_to_check)\n",
    "    existing_agent_dim = existing_agent_dim.withColumn('sales_agent_sur_key', col(\"sales_agent_sur_key\").cast(\"int\"))\n",
    "    existing_agent_dim_without_sk = existing_agent_dim.select('sales_person_id', 'name', 'hire_date')\n",
    "    \n",
    "    new_sales_agent_data = agent_dim.subtract(existing_agent_dim_without_sk)\n",
    "    print(existing_agent_dim)\n",
    "    \n",
    "    # Get the maximum surrogate key from existing data\n",
    "    max_sur_key = existing_agent_dim.agg({\"sales_agent_sur_key\": \"max\"}).collect()[0][0]\n",
    "    print(max_sur_key)\n",
    "    \n",
    "    if not new_sales_agent_data.rdd.isEmpty():\n",
    "        # Add surrogate keys to new data starting from max_sur_key + 1\n",
    "        window_spec = Window.orderBy(\"sales_person_id\")\n",
    "        agent_dim_with_sk = new_sales_agent_data.withColumn('sales_agent_sur_key', (row_number().over(window_spec) + max_sur_key).cast(\"int\"))\n",
    "        \n",
    "        # Combine existing data with new data\n",
    "        updated_agent_dim = existing_agent_dim.union(agent_dim_with_sk)\n",
    "        updated_agent_dim = updated_agent_dim.repartition(1)\n",
    "        \n",
    "        # Write the updated data back to HDFS\n",
    "        updated_agent_dim.write.mode('overwrite') \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .format('parquet') \\\n",
    "            .save(f\"{golden_layer_path}/tmp\")\n",
    "        \n",
    "        rename_in_hdfs(f\"{golden_layer_path}/tmp\", file_extension, name) \n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-rm\", path_to_check])\n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-mv\", f\"{golden_layer_path}/tmp/{name}{file_extension}\", golden_layer_path])\n",
    "    else:\n",
    "        updated_agent_dim = existing_agent_dim\n",
    "        print(\"No new Data\")\n",
    "else:\n",
    "    # Add a sequential surrogate key column\n",
    "    window_spec = Window.orderBy(\"sales_person_id\")\n",
    "    agent_dim_with_sk = agent_dim.withColumn('sales_agent_sur_key', row_number().over(window_spec))\n",
    "    \n",
    "    updated_agent_dim = agent_dim_with_sk\n",
    "    updated_agent_dim = updated_agent_dim.select('sales_agent_sur_key', 'sales_person_id', 'name', 'hire_date')\n",
    "    updated_agent_dim.show()\n",
    "    \n",
    "    # Write the updated data back to HDFS\n",
    "    updated_agent_dim.write.mode('overwrite') \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .format('parquet') \\\n",
    "        .save(golden_layer_path)\n",
    "    \n",
    "    rename_in_hdfs(golden_layer_path, file_extension, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce9bacf-bbee-45a8-a226-5ec4d5f96fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date dimension already exists\n"
     ]
    }
   ],
   "source": [
    "#create date dimension\n",
    "date_dim_path = \"hdfs:///data/retail_gold/date_dim\"\n",
    "name = \"date_dim\"\n",
    "extension = \".parquet\"\n",
    "if not check_if_exists(f\"{date_dim_path}/{name}{extension}\"):\n",
    "    # Generate date range\n",
    "    start_date = date(2012, 1, 1)\n",
    "    end_date = date(2100, 12, 31)\n",
    "\n",
    "    date_range = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "    date_df = spark.createDataFrame([(d,) for d in date_range], [\"datee\"]).withColumn(\"datee\", col(\"datee\").cast(\"date\"))\n",
    "\n",
    "    # Add date attributes\n",
    "    date_dim = date_df.withColumn(\"year\", year(col(\"datee\"))) \\\n",
    "        .withColumn(\"month\", month(col(\"datee\"))) \\\n",
    "        .withColumn(\"day\", dayofmonth(col(\"datee\"))) \\\n",
    "        .withColumn(\"week\", weekofyear(col(\"datee\"))) \\\n",
    "        .withColumn(\"weekday\", dayofweek(col(\"datee\"))) \\\n",
    "        .withColumn(\"quarter\", floor((month(col(\"datee\")) - 1) / 3) + 1) \\\n",
    "        .withColumn(\"day_name\", date_format(col(\"datee\"), \"EEEE\")) \\\n",
    "        .withColumn(\"month_name\", date_format(col(\"datee\"), \"MMMM\")) \\\n",
    "        .withColumn(\"is_weekend\", when(col(\"weekday\").isin([1, 7]), lit(1)).otherwise(lit(0)))\n",
    "\n",
    "    # Add surrogate key column\n",
    "    date_dim = date_dim.withColumn(\"date_sur_key\",concat(col('day'), col('month'), col('year')).cast('long'))\n",
    "\n",
    "    # Define the output directory for the date dimension\n",
    "    date_dim_path = \"hdfs:///data/retail_gold/date_dim\"\n",
    "\n",
    "    try:\n",
    "        # Write the date dimension to a single CSV file\n",
    "        date_dim.repartition(1) \\\n",
    "            .write.mode('overwrite') \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .format('parquet') \\\n",
    "            .save(date_dim_path)\n",
    "        print(f\"Date dimension table saved to {date_dim_path}\")\n",
    "        write_df_to_table('retail_DWH', 'date_dim', date_dim)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # to rename csv file in date dim\n",
    "\n",
    "    rename_in_hdfs(date_dim_path, extension, name)\n",
    "else:\n",
    "    date_dim = spark.read.parquet(f\"{date_dim_path}/{name}{extension}\")\n",
    "    print(\"Date dimension already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d58fda0-7ee8-4069-902c-7e773c867286",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'year is not a valid partition column in table `retail_dwh`.`date_dim`.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o40.sql.\n: org.apache.spark.sql.AnalysisException: year is not a valid partition column in table `retail_dwh`.`date_dim`.;\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$$anonfun$11$$anonfun$13.apply(PartitioningUtils.scala:340)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$$anonfun$11$$anonfun$13.apply(PartitioningUtils.scala:340)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$$anonfun$11.apply(PartitioningUtils.scala:339)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$$anonfun$11.apply(PartitioningUtils.scala:338)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.normalizePartitionSpec(PartitioningUtils.scala:338)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(rules.scala:334)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:376)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:368)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.apply(rules.scala:368)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.apply(rules.scala:328)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:48)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-97a5425d5219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_df_to_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'retail_DWH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date_dim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-d0e3a8cea6ae>\u001b[0m in \u001b[0;36mwrite_df_to_table\u001b[0;34m(database_name, table_name, df, partition_columns, bucket_column, num_buckets)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Execute the insert overwrite query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minsert_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Drop the temporary table after use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'year is not a valid partition column in table `retail_dwh`.`date_dim`.;'"
     ]
    }
   ],
   "source": [
    "#write_df_to_table('retail_DWH', 'date_dim', date_dim, ['year'], 'month', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90a80c64-bfbe-4672-9d23-5130596368ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'updated_customers_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-685f53c298ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfinal_price\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'units'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unit_price'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'discount_perc'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moffline_fact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffline_fact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total_price\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_price\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0moffline_fact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffline_fact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdated_customers_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'customer_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                       \u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdated_product_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'product_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                       \u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_dim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatee\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moffline_fact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransaction_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'updated_customers_dim' is not defined"
     ]
    }
   ],
   "source": [
    " #fact One (offline)\n",
    "#print(input.columns)\n",
    "offline_fact=input_df.filter(col('is_online')==\"no\")\n",
    "columns_to_drop=['shipping_address','customer_fname','cusomter_lname','offer_1','offer_2',\n",
    "                'offer_3','offer_4','offer_5','product_name','product_category','customer_email']\n",
    "\n",
    "offline_fact=offline_fact.drop(*columns_to_drop)\n",
    "#print(offline_fact.columns)\n",
    "#print(offline_fact.take(5))\n",
    "#print(cust_dim.columns)\n",
    "offline_fact=offline_fact.withColumn(\"transaction_date\", col(\"transaction_date\").cast(DateType()))\n",
    "final_price=(col('units') * col('unit_price') * (1 - col('discount_perc') / 100))\n",
    "offline_fact=offline_fact.withColumn(\"total_price\",final_price)\n",
    "offline_fact=offline_fact.join(updated_customers_dim, on='customer_id', how='left') \\\n",
    "                       .join(updated_product_dim, on='product_id', how='left') \\\n",
    "                       .join(date_dim, date_dim.datee == offline_fact.transaction_date, 'left') \\\n",
    "                       .join(updated_agent_dim, updated_agent_dim.sales_person_id == offline_fact.sales_agent_id, 'left') \\\n",
    "                       .join(updated_branches_dim, updated_branches_dim.branch_id == offline_fact.branch_id, 'left')\n",
    "\n",
    "# offline_fact\n",
    "        \n",
    "offline_fact = offline_fact.select(\n",
    "    'transaction_id',\n",
    "    'branch_sur_key',\n",
    "    'product_sur_key',\n",
    "    'customer_sur_key',\n",
    "    'sales_agent_sur_key',\n",
    "    'date_sur_key',\n",
    "    'units',\n",
    "    'unit_price',\n",
    "    'discount_perc',\n",
    "    'total_price',\n",
    "    'payment_method'\n",
    ")\n",
    "\n",
    "fact_off_dim_path=\"hdfs:///data/retail_gold/offline_fact\"\n",
    "name =\"offline_fact\"\n",
    "extension = \".parquet\"\n",
    "full_file_path = f\"{fact_off_dim_path}/{name}{extension}\"\n",
    "if check_if_exists(full_file_path):\n",
    "    old_df = spark.read.parquet(full_file_path)\n",
    "    new_offline_fact = offline_fact.subtract(old_df)\n",
    "    if new_offline_fact.rdd.isEmpty() == False:\n",
    "        new_offline_fact = new_offline_fact.union(offline_fact)\n",
    "        new_offline_fact=new_offline_fact.repartition(1)\n",
    "        new_offline_fact.write.mode('overwrite') \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .format('parquet') \\\n",
    "                    .save(f\"{fact_off_dim_path}/tmp\")\n",
    "        rename_in_hdfs(f\"{fact_off_dim_path}/tmp\",extension,name) \n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-rm\", full_file_path])\n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-mv\", f\"{fact_off_dim_path}/tmp/{name}{extension}\" ,fact_off_dim_path])\n",
    "        print(\"done\")\n",
    "    else:\n",
    "        print(\"No new Data\")\n",
    "    \n",
    "else:\n",
    "    offline_fact=offline_fact.repartition(1)\n",
    "    offline_fact.write.mode('overwrite') \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .format('parquet') \\\n",
    "                .save(fact_off_dim_path)\n",
    "\n",
    "    rename_in_hdfs(fact_off_dim_path,extension,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0547af3-6762-40ca-b393-edfb56ef927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File moved and renamed to: hdfs:///data/retail_gold/online_fact/online_fact.parquet\n"
     ]
    }
   ],
   "source": [
    "# online_fact\n",
    "online_fact = input_df.filter(col('is_online')==\"yes\")\n",
    "\n",
    "columns_to_drop = ['customer_fname','cusomter_lname','sales_agent_id','offer_1','offer_2',\n",
    "                'offer_3','offer_4','offer_5','product_name','product_category','customer_email']\n",
    "\n",
    "online_fact = online_fact.drop(*columns_to_drop)\n",
    "# print(online_fact.columns)\n",
    "\n",
    "# cast transaction_date to date type \n",
    "online_fact=online_fact.withColumn(\"transaction_date\", col(\"transaction_date\").cast(DateType()))\n",
    "\n",
    "# calculate the final price \n",
    "final_price=(col('units') * col('unit_price') * (1 - (col('discount_perc') / 100)))\n",
    "online_fact=online_fact.withColumn(\"total_price\",final_price)\n",
    "\n",
    "#processing address column\n",
    "split_address_col=split(col(\"shipping_address\"),'/')\n",
    "online_fact=online_fact.withColumn('street',split_address_col.getItem(0))\\\n",
    "                        .withColumn('city',split_address_col.getItem(1))\\\n",
    "                        .withColumn('state',split_address_col.getItem(2))\\\n",
    "                        .withColumn('postal_code',split_address_col.getItem(3))\n",
    "\n",
    "# Join with dimension tables using left join\n",
    "online_fact = online_fact.join(updated_customers_dim, on='customer_id', how='left') \\\n",
    "                         .join(updated_product_dim, on='product_id', how='left') \\\n",
    "                         .join(date_dim, date_dim.datee == online_fact.transaction_date, 'left')\n",
    "# online_fact\n",
    "\n",
    "online_fact = online_fact.select(\n",
    "    'transaction_id',\n",
    "    'units',\n",
    "    'unit_price',\n",
    "    'payment_method',\n",
    "    'discount_perc',\n",
    "    'total_price',\n",
    "    'customer_sur_key',\n",
    "    'product_sur_key',\n",
    "    'date_sur_key',\n",
    "    'street',\n",
    "    'city',\n",
    "    'state',\n",
    "    'postal_code'\n",
    ")\n",
    "\n",
    "online_fact_path=\"hdfs:///data/retail_gold/online_fact\"\n",
    "name=\"online_fact\"\n",
    "extension = \".parquet\"\n",
    "full_file_path = f\"{online_fact_path}/{name}{extension}\"\n",
    "\n",
    "if check_if_exists(full_file_path):\n",
    "    old_df = spark.read.parquet(full_file_path)\n",
    "    new_online_fact = online_fact.subtract(old_df)\n",
    "    if new_online_fact.rdd.isEmpty() == False:\n",
    "        new_online_fact = new_online_fact.union(online_fact)\n",
    "        new_online_fact=new_online_fact.repartition(1)\n",
    "        new_online_fact.write.mode('overwrite') \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .format('parquet') \\\n",
    "                    .save(f\"{online_fact_path}/tmp\")\n",
    "        rename_in_hdfs(f\"{online_fact_path}/tmp\",extension,name) \n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-rm\", full_file_path])\n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-mv\", f\"{online_fact_path}/tmp/{name}{extension}\" ,online_fact_path])\n",
    "        print(\"done\")\n",
    "        \n",
    "    \n",
    "else:\n",
    "    online_fact=online_fact.repartition(1)\n",
    "    online_fact.write.mode('overwrite') \\\n",
    "             .option(\"header\", \"true\") \\\n",
    "             .format('parquet') \\\n",
    "             .save(online_fact_path)\n",
    "    rename_in_hdfs(online_fact_path,extension,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "958a4b08-685a-48e2-80fc-886119f58657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_sur_key: integer (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_fname: string (nullable = true)\n",
      " |-- cusomter_lname: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_customers_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d31a8dce-4533-40d3-8ed1-4f5a992eb308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_sur_key: integer (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_product_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "062bea90-867f-4858-868b-ee411fa648f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- quarter: long (nullable = true)\n",
      " |-- day_name: string (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- is_weekend: integer (nullable = false)\n",
      " |-- date_sur_key: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2a6c1d9-1d96-42b1-96ee-3bf39c991af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- discount_perc: integer (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- customer_sur_key: integer (nullable = true)\n",
      " |-- product_sur_key: integer (nullable = true)\n",
      " |-- date_sur_key: long (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_fact.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a32ace1a-d742-4f75-bac2-95a406876b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- branch_sur_key: integer (nullable = true)\n",
      " |-- product_sur_key: integer (nullable = true)\n",
      " |-- customer_sur_key: integer (nullable = true)\n",
      " |-- sales_agent_sur_key: integer (nullable = true)\n",
      " |-- date_sur_key: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount_perc: integer (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "offline_fact.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2a697b5-e4a6-4061-bea7-2fee448e60d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sales_agent_sur_key: integer (nullable = true)\n",
      " |-- sales_person_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_agent_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ae9ace6-3f49-4d59-8a82-f22ffce7db57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- branch_sur_key: integer (nullable = true)\n",
      " |-- branch_id: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- establish_date: date (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_branches_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91e964d5-08c9-49d8-8853-a5cdc2941f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------------+--------------+--------------------+\n",
      "|customer_sur_key|customer_id|customer_fname|cusomter_lname|      customer_email|\n",
      "+----------------+-----------+--------------+--------------+--------------------+\n",
      "|               1|      85462|        Olivia|         Brown|olivia.brown@yaho...|\n",
      "|               2|      85463|           Mia|      Williams|mia.williams@gmai...|\n",
      "|               3|      85464|          Emma|      Williams|emma.williams@out...|\n",
      "|               4|      85465|         James|        Taylor|james.taylor@gmai...|\n",
      "|               5|      85466|       Michael|         Brown|michael.brown@yah...|\n",
      "|               6|      85467|     Alexander|         Jones|alexander.jones@y...|\n",
      "|               7|      85468|       William|         Davis|william.davis@yah...|\n",
      "|               8|      85469|     Alexander|         Brown|alexander.brown@g...|\n",
      "|               9|      85470|           Ava|        Wilson|ava.wilson@hotmai...|\n",
      "|              10|      85471|           Ava|      Williams|ava.williams@outl...|\n",
      "|              11|      85472|           Ava|         Smith|ava.smith@outlook...|\n",
      "|              12|      85473|        Sophia|        Miller|sophia.miller@yah...|\n",
      "|              13|      85474|          Emma|       Johnson|emma.johnson@gmai...|\n",
      "|              14|      85475|         James|       Johnson|james.johnson@gma...|\n",
      "|              15|      85476|         James|         Moore|james.moore@yahoo...|\n",
      "|              16|      85477|     Alexander|      Williams|alexander.william...|\n",
      "|              17|      85478|           Mia|         Davis| mia.davis@gmail.com|\n",
      "|              18|      85479|         James|         Jones|james.jones@outlo...|\n",
      "|              19|      85480|     Alexander|      Williams|alexander.william...|\n",
      "|              20|      85481|       Michael|         Moore|michael.moore@out...|\n",
      "+----------------+-----------+--------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- customer_sur_key: integer (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_fname: string (nullable = true)\n",
      " |-- cusomter_lname: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust = spark.read.parquet(\"hdfs:///data/retail_gold/customer_dim/\")\n",
    "cust.show()\n",
    "cust.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a25340-9ea5-403d-a2f7-2cd58aa843c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
