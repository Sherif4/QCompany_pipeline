{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a40387",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [15]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0a705e-d013-4349-b873-78fcb1d1cda5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:11.689373Z",
     "iopub.status.busy": "2024-07-12T22:26:11.686707Z",
     "iopub.status.idle": "2024-07-12T22:26:11.890115Z",
     "shell.execute_reply": "2024-07-12T22:26:11.889775Z"
    },
    "papermill": {
     "duration": 0.253097,
     "end_time": "2024-07-12T22:26:11.890202",
     "exception": false,
     "start_time": "2024-07-12T22:26:11.637105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " import re\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, regexp_replace, trim,when ,monotonically_increasing_id,lit,year, month, dayofmonth, weekofyear, dayofweek, date_format,floor,dense_rank,\\\n",
    "substring,concat,split, row_number,lpad,lit, current_date\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
    "from datetime import date, datetime, timedelta\n",
    "import subprocess\n",
    "from py4j.java_gateway import java_import\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbd3353-908f-4d90-850f-c36424d80817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:11.931437Z",
     "iopub.status.busy": "2024-07-12T22:26:11.930946Z",
     "iopub.status.idle": "2024-07-12T22:26:16.334525Z",
     "shell.execute_reply": "2024-07-12T22:26:16.334179Z"
    },
    "papermill": {
     "duration": 4.426584,
     "end_time": "2024-07-12T22:26:16.334619",
     "exception": false,
     "start_time": "2024-07-12T22:26:11.908035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .appName(\"Gold_layer_transformation\")\\\n",
    "    .config(\"spark.eventLog.logBlockUpdates.enabled\", True)\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f347ff1-f802-41f3-975d-9e15f235b4d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:16.388470Z",
     "iopub.status.busy": "2024-07-12T22:26:16.388035Z",
     "iopub.status.idle": "2024-07-12T22:26:18.232076Z",
     "shell.execute_reply": "2024-07-12T22:26:18.231612Z"
    },
    "papermill": {
     "duration": 1.869982,
     "end_time": "2024-07-12T22:26:18.232176",
     "exception": false,
     "start_time": "2024-07-12T22:26:16.362194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>key</th><th>value</th></tr>\n",
       "<tr><td>hive.enforce.buck...</td><td>true</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+-----+\n",
       "|                 key|value|\n",
       "+--------------------+-----+\n",
       "|hive.enforce.buck...| true|\n",
       "+--------------------+-----+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    " spark.sql(\"set hive.enforce.bucketing=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6120cae5-dc75-4881-b2c2-139445c22d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:18.277158Z",
     "iopub.status.busy": "2024-07-12T22:26:18.276719Z",
     "iopub.status.idle": "2024-07-12T22:26:18.279521Z",
     "shell.execute_reply": "2024-07-12T22:26:18.279156Z"
    },
    "papermill": {
     "duration": 0.026535,
     "end_time": "2024-07-12T22:26:18.279603",
     "exception": false,
     "start_time": "2024-07-12T22:26:18.253068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "date_str = now.strftime(\"%Y%m%d\")\n",
    "hour_str = now.strftime(\"%H\")\n",
    "path = f\"hdfs:///data/retail_silver/{date_str}/{hour_str}/sales_transactions_SS_cleaned_{date_str}_{hour_str}.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf8fd60-448d-4761-9e0c-c69f3851b8ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:18.327071Z",
     "iopub.status.busy": "2024-07-12T22:26:18.325433Z",
     "iopub.status.idle": "2024-07-12T22:26:20.427058Z",
     "shell.execute_reply": "2024-07-12T22:26:20.427504Z"
    },
    "papermill": {
     "duration": 2.128938,
     "end_time": "2024-07-12T22:26:20.427721",
     "exception": false,
     "start_time": "2024-07-12T22:26:18.298783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if os.system(f\"hdfs dfs -test -e {path}\") == 0:\n",
    "        input_df = spark.read.parquet(path)\n",
    "    else:\n",
    "        raise SystemExit(f\"Path does not exist: {path}\")  # Exit with code 1 for missing path\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise SystemExit(1)  # Exit with code 1 for other errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d197f3-24a9-47f2-9bc5-566b12afe3da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:20.478583Z",
     "iopub.status.busy": "2024-07-12T22:26:20.478074Z",
     "iopub.status.idle": "2024-07-12T22:26:20.481661Z",
     "shell.execute_reply": "2024-07-12T22:26:20.481260Z"
    },
    "papermill": {
     "duration": 0.03042,
     "end_time": "2024-07-12T22:26:20.481807",
     "exception": false,
     "start_time": "2024-07-12T22:26:20.451387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #function_to_rename_in_hdfs\n",
    "# def rename_in_hdfs(golden_layer_path,file_extension,name):\n",
    "#     # Run the Hadoop fs -ls command to list files\n",
    "#     list_files_process = subprocess.run([\"hadoop\", \"fs\", \"-ls\", golden_layer_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "#     # Check for errors\n",
    "#     if list_files_process.returncode != 0:\n",
    "#         print(f\"Error listing files in {golden_layer_path}: {list_files_process.stderr.decode()}\")\n",
    "#         exit(1)\n",
    "\n",
    "#     # Decode stdout to string format and split lines\n",
    "#     stdout_str = list_files_process.stdout.decode()\n",
    "#     file_list = stdout_str.splitlines()\n",
    "\n",
    "#     # Find the file to rename based on criteria\n",
    "#     file_to_rename = None\n",
    "#     for line in file_list:\n",
    "#         if line.endswith(file_extension):\n",
    "#             file_to_rename = line.split()[-1].strip()\n",
    "#             break\n",
    "\n",
    "#     # Check if a file matching the criteria was found\n",
    "#     if file_to_rename:\n",
    "#         new_filename = f\"{golden_layer_path}/{name}{file_extension}\"\n",
    "\n",
    "#         # Move (rename) the file\n",
    "#         subprocess.run([\"hadoop\", \"fs\", \"-mv\", file_to_rename, new_filename])\n",
    "\n",
    "#         print(f\"File moved and renamed to: {new_filename}\")\n",
    "#     else:\n",
    "#         print(\"File matching the criteria not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286a9c5-2ad2-4c24-b49b-ff196f2ddbb3",
   "metadata": {
    "papermill": {
     "duration": 0.020053,
     "end_time": "2024-07-12T22:26:20.522659",
     "exception": false,
     "start_time": "2024-07-12T22:26:20.502606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Function to check if a file exists in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38c7b45-96e0-4758-ac81-ae7461673030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:20.566523Z",
     "iopub.status.busy": "2024-07-12T22:26:20.566004Z",
     "iopub.status.idle": "2024-07-12T22:26:20.570421Z",
     "shell.execute_reply": "2024-07-12T22:26:20.569930Z"
    },
    "papermill": {
     "duration": 0.027912,
     "end_time": "2024-07-12T22:26:20.570511",
     "exception": false,
     "start_time": "2024-07-12T22:26:20.542599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_if_exists(path):\n",
    "    jvm = spark._jvm\n",
    "    jsc = spark._jsc\n",
    "    fs = jvm.org.apache.hadoop.fs.FileSystem.get(jsc.hadoopConfiguration())\n",
    "    return fs.exists(jvm.org.apache.hadoop.fs.Path(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfa2dd-6f83-42e4-8a1a-ef890946e254",
   "metadata": {
    "papermill": {
     "duration": 0.018767,
     "end_time": "2024-07-12T22:26:20.609144",
     "exception": false,
     "start_time": "2024-07-12T22:26:20.590377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A function that writes a df to hive external_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06701e99-1036-47c3-a789-455f56bc216f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:20.651978Z",
     "iopub.status.busy": "2024-07-12T22:26:20.651452Z",
     "iopub.status.idle": "2024-07-12T22:26:20.661591Z",
     "shell.execute_reply": "2024-07-12T22:26:20.661243Z"
    },
    "papermill": {
     "duration": 0.033648,
     "end_time": "2024-07-12T22:26:20.661682",
     "exception": false,
     "start_time": "2024-07-12T22:26:20.628034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_df_to_table(database_name, table_name, df, partition_columns=None):\n",
    "    # Temporary table\n",
    "    temp_table = \"temp_table\"\n",
    "\n",
    "    # Create DataFrameWriter with overwrite mode\n",
    "    writer = df.write.mode(\"overwrite\")\n",
    "\n",
    "    # If partition columns are provided, specify them in the writer\n",
    "    if partition_columns:\n",
    "        if isinstance(partition_columns, list):\n",
    "            writer = writer.partitionBy(*partition_columns)\n",
    "        else:\n",
    "            writer = writer.partitionBy(partition_columns)\n",
    "\n",
    "    # Save the DataFrame to a temporary table\n",
    "    writer.saveAsTable(temp_table)\n",
    "\n",
    "    # Construct the insert overwrite query\n",
    "    if partition_columns:\n",
    "        if isinstance(partition_columns, list):\n",
    "            partition_str = \", \".join(partition_columns)\n",
    "        else:\n",
    "            partition_str = partition_columns\n",
    "    else:\n",
    "        partition_str = \"\"\n",
    "\n",
    "    insert_query = f\"\"\"\n",
    "         INSERT INTO TABLE {database_name}.{table_name}\n",
    "         {f'PARTITION ({partition_str})' if partition_str else ''}\n",
    "         SELECT * FROM {temp_table}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Execute the insert overwrite query\n",
    "        spark.sql(insert_query)\n",
    "\n",
    "        # Drop the temporary table after use\n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-rm\", \"-r\", '/user/hive/warehouse/temp_table'])\n",
    "        print(f\"Data written successfully to {database_name}.{table_name}\")\n",
    "    except Exception as e:\n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-rm\", \"-r\", '/user/hive/warehouse/temp_table'])\n",
    "        print(f\"Error in writing to {table_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bba9bfe8-67ad-4dcc-afdd-79f262fac181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:20.717778Z",
     "iopub.status.busy": "2024-07-12T22:26:20.717059Z",
     "iopub.status.idle": "2024-07-12T22:26:35.703011Z",
     "shell.execute_reply": "2024-07-12T22:26:35.703582Z"
    },
    "papermill": {
     "duration": 15.021117,
     "end_time": "2024-07-12T22:26:35.704352",
     "exception": false,
     "start_time": "2024-07-12T22:26:20.683235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written successfully to retail_DWH.Customer_Dim\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#write customer dim in HDFS\n",
    "#input_df = spark.read.parquet(f\"hdfs:///data/retail_silver/20240712/19/sales_transactions_SS_cleaned_20240712_19.parquet\")\n",
    "\n",
    "cust_data = input_df.select('customer_id', 'customer_fname', 'customer_lname', 'customer_email')\n",
    "golden_layer_path=\"hdfs://localhost:9000/data/retail_gold/customer_dim\"\n",
    "\n",
    "if check_if_exists(golden_layer_path):\n",
    "    cust_dim = spark.read.parquet(golden_layer_path)\n",
    "    existing_cust_dim_without_sk = cust_dim.select('customer_id', 'customer_fname', 'customer_lname', 'customer_email','hashkey')\n",
    "    all_cols = F.concat_ws(\"\", *cust_data.columns)\n",
    "    new_customers_data = cust_data.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    new_customers_data = new_customers_data.dropDuplicates(['hashkey'])\n",
    "    new_customers_data = new_customers_data.join(existing_cust_dim_without_sk, \"hashkey\", \"left_anti\")\n",
    "    # Get the maximum surrogate key from existing data\n",
    "    max_sur_key = cust_dim.agg({\"customer_sur_key\": \"max\"}).collect()[0][0]\n",
    "    \n",
    "    # Combine existing data with new data\n",
    "    if new_customers_data.rdd.isEmpty() == False:\n",
    "        window_spec = Window.orderBy(\"customer_id\")\n",
    "        #Add surrogate keys to new data starting from max_sur_key + 1\n",
    "        new_customers_data = new_customers_data.withColumn('customer_sur_key', (row_number().over(window_spec) + max_sur_key).cast(\"int\"))\n",
    "        customers_dim_sk = new_customers_data.select('customer_sur_key','customer_id', 'customer_fname', 'customer_lname', 'customer_email')\n",
    "        existing_cust_dim_without_hash = cust_dim.drop(\"hashkey\")\n",
    "        #write the new data on its location on the gold layer\n",
    "        new_customers_data = new_customers_data.select('customer_sur_key','customer_id', 'customer_fname', 'customer_lname', 'customer_email','hashkey')\n",
    "        customers_dim = new_customers_data.repartition(1)\n",
    "        customers_dim.write.mode('append') \\\n",
    "                            .format('parquet') \\\n",
    "                            .save(golden_layer_path)\n",
    "        #write the new data into the customer dimension hive table\n",
    "        cust_dim = spark.read.parquet(golden_layer_path)\n",
    "        updated_customers_dim = cust_data.join(cust_dim, on=\n",
    "                                               [cust_data.customer_id.alias('2')==cust_dim.customer_id, cust_data.customer_email==cust_dim.customer_email],\n",
    "                                              how='left')\n",
    "        updated_customers_dim = updated_customers_dim.select('customer_sur_key', cust_data.customer_id.alias('customer_id'), cust_data.customer_fname.alias('customer_fname'), cust_data.customer_lname.alias('customer_lname'), cust_data.customer_email.alias('customer_email'))\n",
    "        write_df_to_table('retail_DWH', 'Customer_Dim', customers_dim_sk)\n",
    "        print(\"done\")\n",
    "    else:\n",
    "        updated_customers_dim = cust_dim.drop(\"hashkey\")\n",
    "        print(\"No new Data\")\n",
    "else:\n",
    "    # Add a sequential surrogate key column\n",
    "    cust_data = cust_data.dropDuplicates(['customer_id'])\n",
    "    all_cols = F.concat_ws(\"\", *cust_data.columns)\n",
    "    hashed_customer_dim = cust_data.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    window_spec = Window.orderBy(\"customer_id\")\n",
    "    hashed_customer_dim = hashed_customer_dim.withColumn('customer_sur_key', row_number().over(window_spec))\n",
    "    hashed_customer_dim = hashed_customer_dim.select('customer_sur_key','customer_id', 'customer_fname', 'customer_lname', 'customer_email','hashkey')\n",
    "    #make customer dim \n",
    "    updated_customers_dim = hashed_customer_dim.select('customer_sur_key','customer_id', 'customer_fname', 'customer_lname', 'customer_email')\n",
    "\n",
    "    hashed_customer_dim.repartition(1)\n",
    "    hashed_customer_dim.write.mode('overwrite') \\\n",
    "            .format('parquet') \\\n",
    "            .save(golden_layer_path)\n",
    "    updated_customers_dim.show(5)\n",
    "    #write the new data into the customer dimension hive table\n",
    "    write_df_to_table('retail_DWH', 'Customer_Dim', updated_customers_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff04be8c-f132-4f0a-99f4-0474f7806761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:35.774209Z",
     "iopub.status.busy": "2024-07-12T22:26:35.773800Z",
     "iopub.status.idle": "2024-07-12T22:26:39.734037Z",
     "shell.execute_reply": "2024-07-12T22:26:39.733675Z"
    },
    "papermill": {
     "duration": 3.997451,
     "end_time": "2024-07-12T22:26:39.734127",
     "exception": false,
     "start_time": "2024-07-12T22:26:35.736676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new Data\n"
     ]
    }
   ],
   "source": [
    "# Ensure product_dim is distinct by product_id and add a sequential surrogate key\n",
    "# Select relevant columns for product_dim\n",
    "product_dim = input_df.select('product_id', 'product_name', 'product_category')\n",
    "\n",
    "# Drop duplicates based on product_id if necessary\n",
    "product_dim = product_dim.dropDuplicates(['product_id','product_name','product_category'])\n",
    "\n",
    "# Define the golden layer path and file details\n",
    "golden_layer_path = \"hdfs://localhost:9000/data/retail_gold/product_dim/\"\n",
    "\n",
    "if check_if_exists(golden_layer_path):\n",
    "    #Hashing the records and compare the old and new dataframes\n",
    "    existing_product_dim = spark.read.parquet(golden_layer_path)\n",
    "    existing_product_dim = existing_product_dim.withColumn(\"product_sur_key\", col(\"product_sur_key\").cast(\"int\"))\n",
    "    existing_product_dim_without_sk = existing_product_dim.select('product_id', 'product_name', 'product_category', 'hashkey')\n",
    "    all_cols = F.concat_ws(\"\", *product_dim.columns)\n",
    "    new_products_data = product_dim.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    new_products_data = new_products_data.join(existing_product_dim_without_sk, \"hashkey\", \"left_anti\")\n",
    "    \n",
    "    # Combine existing data with new data\n",
    "    if new_products_data.rdd.isEmpty() == False:\n",
    "        # Get the maximum surrogate key from existing data\n",
    "        max_sur_key = existing_product_dim.agg({\"product_sur_key\": \"max\"}).collect()[0][0]\n",
    "        window_spec = Window.orderBy(\"product_id\")\n",
    "        product_dim = new_products_data.withColumn('product_sur_key', (row_number().over(window_spec) + max_sur_key).cast(\"int\"))\n",
    "        product_dim_sk = product_dim.select('product_sur_key','product_id', 'product_name', 'product_category')\n",
    "        existing_product_dim_without_hash = existing_product_dim.drop(\"hashkey\")\n",
    "        updated_product_dim = existing_product_dim_without_hash.union(product_dim_sk)\n",
    "        product_dim = product_dim.select('product_sur_key','product_id', 'product_name', 'product_category','hashkey')\n",
    "        product_dim = product_dim.repartition(1)\n",
    "        product_dim.write.mode('append') \\\n",
    "                            .format('parquet') \\\n",
    "                            .save(golden_layer_path)\n",
    "        \n",
    "        write_df_to_table('retail_DWH', 'Product_Dim', product_dim_sk)\n",
    "    else:\n",
    "        updated_product_dim = existing_product_dim.drop(\"hashkey\")\n",
    "        print(\"No new Data\")\n",
    "else:\n",
    "    # Add a sequential surrogate key column\n",
    "    window_spec = Window.orderBy(\"product_id\")\n",
    "    all_cols = F.concat_ws(\"\", *product_dim.columns)\n",
    "    hashed_product_dim = product_dim.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    hashed_product_dim = hashed_product_dim.withColumn('product_sur_key', row_number().over(window_spec))\n",
    "    hashed_product_dim = hashed_product_dim.select('product_sur_key','product_id', 'product_name', 'product_category','hashkey')\n",
    "    updated_product_dim = hashed_product_dim.select('product_sur_key','product_id', 'product_name', 'product_category')\n",
    "\n",
    "    updated_product_dim.show()\n",
    "    \n",
    "    # Repartition to one file for efficient writing\n",
    "    hashed_product_dim = hashed_product_dim.repartition(1)\n",
    "\n",
    "    # Write the updated data back to HDFS\n",
    "    hashed_product_dim.write.mode('overwrite') \\\n",
    "        .format('parquet') \\\n",
    "        .save(golden_layer_path)\n",
    "    \n",
    "    write_df_to_table('retail_DWH', 'Product_Dim', updated_product_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f04e839-8863-448a-90e5-13e1ff09c5fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:39.808594Z",
     "iopub.status.busy": "2024-07-12T22:26:39.807864Z",
     "iopub.status.idle": "2024-07-12T22:26:43.592258Z",
     "shell.execute_reply": "2024-07-12T22:26:43.591954Z"
    },
    "papermill": {
     "duration": 3.832737,
     "end_time": "2024-07-12T22:26:43.592346",
     "exception": false,
     "start_time": "2024-07-12T22:26:39.759609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new Data\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for the initial CSV data and the golden layer path on HDFS\n",
    "file_path = f\"hdfs://localhost:9000/data/retail_bronze/{date_str}/08/branches_SS_raw_{date_str}_08.parquet\"\n",
    "golden_layer_path = \"hdfs://localhost:9000/data/retail_gold/branches_dim\"\n",
    "\n",
    "# Load the CSV data into a PySpark DataFrame\n",
    "branches_dim = spark.read.parquet(file_path)\n",
    "\n",
    "# Convert establish_date to date type if needed\n",
    "branches_dim = branches_dim.withColumn(\"establish_date\", col(\"establish_date\").cast(\"date\"))\n",
    "\n",
    "# Drop duplicates based on branch_id if necessary\n",
    "branches_dim = branches_dim.dropDuplicates(['branch_id'])\n",
    "\n",
    "if check_if_exists(golden_layer_path):\n",
    "    #Hashing the records and compare the old and new dataframes\n",
    "    existing_branch_dim = spark.read.parquet(golden_layer_path)\n",
    "    existing_branch_dim_without_sk = existing_branch_dim.select('branch_id', 'location', 'establish_date', 'class', 'hashkey')\n",
    "    all_cols = F.concat_ws(\"\", *branches_dim.columns)\n",
    "    new_branches_data = branches_dim.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    new_branches_data = new_branches_data.join(existing_branch_dim_without_sk, \"hashkey\", \"left_anti\")\n",
    "        \n",
    "    if new_branches_data.rdd.isEmpty() == False:\n",
    "        # Get the maximum surrogate key from existing data\n",
    "        max_sur_key = existing_branch_dim.agg({\"branch_sur_key\": \"max\"}).collect()[0][0]\n",
    "        window_spec = Window.orderBy(\"branch_id\")\n",
    "        # Add surrogate keys to new data starting from max_sur_key + 1\n",
    "        branches_dim = new_branches_data.withColumn('branch_sur_key', (row_number().over(window_spec) + max_sur_key).cast(\"int\"))\n",
    "        branches_dim_sk = branches_dim.select('branch_sur_key','branch_id', 'location', 'establish_date', 'class')\n",
    "        # Combine existing data with new data\n",
    "        existing_branch_dim_without_hash = existing_branch_dim.drop(\"hashkey\")\n",
    "        updated_branches_dim = existing_branch_dim_without_hash.union(branches_dim_sk)\n",
    "        branches_dim = branches_dim.select('branch_sur_key','branch_id', 'location', 'establish_date', 'class','hashkey')\n",
    "        branches_dim = branches_dim.repartition(1)\n",
    "        branches_dim.write.mode('append') \\\n",
    "                    .format('parquet') \\\n",
    "                    .save(golden_layer_path)\n",
    "        #write the new data on the branches dimension hive table\n",
    "        write_df_to_table('retail_DWH', 'Branches_Dim', branches_dim_sk)\n",
    "    else:\n",
    "        updated_branches_dim = existing_branch_dim.drop(\"hashkey\")\n",
    "        print(\"No new Data\")\n",
    "else:\n",
    "    # Add a sequential surrogate key column\n",
    "    all_cols = F.concat_ws(\"\", *branches_dim.columns)\n",
    "    hashed_branch_dim = branches_dim.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    window_spec = Window.orderBy(\"branch_id\")\n",
    "    hashed_branch_dim = hashed_branch_dim.withColumn('branch_sur_key', row_number().over(window_spec))\n",
    "    hashed_branch_dim = hashed_branch_dim.select('branch_sur_key','branch_id', 'location', 'establish_date', 'class','hashkey')\n",
    "    updated_branches_dim = hashed_branch_dim.select('branch_sur_key', 'branch_id', 'location', 'establish_date', 'class')\n",
    "\n",
    "    updated_branches_dim.show()\n",
    "    #write the new data on the branches dimension hive table\n",
    "    write_df_to_table('retail_DWH', 'Branches_Dim', updated_branches_dim)\n",
    "    # Write the updated data back to HDFS\n",
    "    branches_dim.repartition(1)\n",
    "    branches_dim.write.mode('overwrite') \\\n",
    "        .format('parquet') \\\n",
    "        .save(golden_layer_path)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c9bafc-3338-4fd9-bc8f-1797874a62bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:43.652040Z",
     "iopub.status.busy": "2024-07-12T22:26:43.651608Z",
     "iopub.status.idle": "2024-07-12T22:26:47.342703Z",
     "shell.execute_reply": "2024-07-12T22:26:47.342240Z"
    },
    "papermill": {
     "duration": 3.724182,
     "end_time": "2024-07-12T22:26:47.342845",
     "exception": false,
     "start_time": "2024-07-12T22:26:43.618663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new Data\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for the initial CSV data and the golden layer path on HDFS\n",
    "file_path = f\"hdfs://localhost:9000/data/retail_bronze/{date_str}/08/sales_agents_SS_raw_{date_str}_08.parquet\"\n",
    "golden_layer_path = \"hdfs://localhost:9000/data/retail_gold/sales_agent_dim\"\n",
    "\n",
    "# Load the CSV data into a PySpark DataFrame\n",
    "agent_dim = spark.read.parquet(file_path)\n",
    "\n",
    "# Convert hire_date to date type if needed\n",
    "agent_dim = agent_dim.withColumn(\"hire_date\", col(\"hire_date\").cast(\"date\"))\n",
    "\n",
    "# Drop duplicates based on sales_person_id if necessary\n",
    "agent_dim = agent_dim.dropDuplicates(['sales_person_id'])\n",
    "\n",
    "if check_if_exists(golden_layer_path):\n",
    "    #Hashing the records and compare the old and new dataframes\n",
    "    existing_agent_dim = spark.read.parquet(golden_layer_path)\n",
    "    existing_agent_dim = existing_agent_dim.withColumn('sales_agent_sur_key', col(\"sales_agent_sur_key\").cast(\"int\"))\n",
    "    existing_agent_dim_without_sk = existing_agent_dim.select('sales_person_id', 'name', 'hire_date', 'hashkey')\n",
    "    all_cols = F.concat_ws(\"\", *agent_dim.columns)\n",
    "    new_sales_agent_data = agent_dim.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    new_sales_agent_data = new_sales_agent_data.join(existing_agent_dim_without_sk, \"hashkey\", \"left_anti\")\n",
    "    \n",
    "    if not new_sales_agent_data.rdd.isEmpty():\n",
    "        # Get the maximum surrogate key from existing data\n",
    "        max_sur_key = existing_agent_dim.agg({\"sales_agent_sur_key\": \"max\"}).collect()[0][0]\n",
    "        print(max_sur_key)\n",
    "        # Add surrogate keys to new data starting from max_sur_key + 1\n",
    "        window_spec = Window.orderBy(\"sales_person_id\")\n",
    "        new_sales_agent_data = new_sales_agent_data.withColumn('sales_agent_sur_key', (row_number().over(window_spec) + max_sur_key).cast(\"int\"))\n",
    "        agent_dim_with_sk = agent_dim.select('sales_agent_sur_key','sales_person_id', 'name', 'hire_date')\n",
    "        # Combine existing data with new data\n",
    "        existing_agent_dim_without_hash = existing_agent_dim.drop(\"hashkey\")\n",
    "        updated_agent_dim = existing_agent_dim_without_hash.union(agent_dim_with_sk)\n",
    "        new_sales_agent_data = new_sales_agent_data.select('sales_agent_sur_key','sales_person_id', 'name', 'hire_date', 'hashkey')\n",
    "        new_sales_agent_data = new_sales_agent_data.repartition(1)\n",
    "        \n",
    "        # Write the new data back to HDFS\n",
    "        new_sales_agent_data.write.mode('append') \\\n",
    "            .format('parquet') \\\n",
    "            .save(golden_layer_path)\n",
    "        #write the new data on the sales agent dimension hive table\n",
    "        write_df_to_table('retail_DWH', 'sales_agents_Dim', agent_dim_with_sk)\n",
    "    else:\n",
    "        updated_agent_dim = existing_agent_dim.drop(\"hashkey\")\n",
    "        print(\"No new Data\")\n",
    "else:\n",
    "    all_cols = F.concat_ws(\"\", *agent_dim.columns)\n",
    "    hashed_agent_dim = agent_dim.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    # Add a sequential surrogate key column\n",
    "    window_spec = Window.orderBy(\"sales_person_id\")\n",
    "    hashed_agent_dim = hashed_agent_dim.withColumn('sales_agent_sur_key', row_number().over(window_spec))\n",
    "    \n",
    "    updated_agent_dim = hashed_agent_dim.select('sales_agent_sur_key', 'sales_person_id', 'name', 'hire_date')\n",
    "    updated_agent_dim.show()\n",
    "    hashed_agent_dim=hashed_agent_dim.select('sales_agent_sur_key','sales_person_id', 'name', 'hire_date', 'hashkey')\n",
    "    # Write the updated data back to HDFS\n",
    "    hashed_agent_dim.repartition(1)\n",
    "    hashed_agent_dim.write.mode('overwrite') \\\n",
    "        .format('parquet') \\\n",
    "        .save(golden_layer_path)\n",
    "    #write the new data on the sales agent dimension hive table\n",
    "    write_df_to_table('retail_DWH', 'sales_agents_Dim', updated_agent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ce9bacf-bbee-45a8-a226-5ec4d5f96fb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:47.402639Z",
     "iopub.status.busy": "2024-07-12T22:26:47.401843Z",
     "iopub.status.idle": "2024-07-12T22:26:47.505509Z",
     "shell.execute_reply": "2024-07-12T22:26:47.505917Z"
    },
    "papermill": {
     "duration": 0.138992,
     "end_time": "2024-07-12T22:26:47.506039",
     "exception": false,
     "start_time": "2024-07-12T22:26:47.367047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date dimension already exists\n"
     ]
    }
   ],
   "source": [
    "#create date dimension\n",
    "date_dim_path = \"hdfs://localhost:9000/data/retail_gold/date_dim\"\n",
    "\n",
    "if not check_if_exists(date_dim_path):\n",
    "    # Generate date range\n",
    "    start_date = date(2012, 1, 1)\n",
    "    end_date = date(2100, 12, 31)\n",
    "\n",
    "    date_range = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "    date_df = spark.createDataFrame([(d,) for d in date_range], [\"datee\"]).withColumn(\"datee\", col(\"datee\").cast(\"date\"))\n",
    "\n",
    "    # Add date attributes\n",
    "    date_dim = date_df.withColumn(\"year\", year(col(\"datee\"))) \\\n",
    "        .withColumn(\"month\", month(col(\"datee\"))) \\\n",
    "        .withColumn(\"day\", lpad(dayofmonth(col(\"datee\")), 2, \"0\")) \\\n",
    "        .withColumn(\"week\", weekofyear(col(\"datee\"))) \\\n",
    "        .withColumn(\"weekday\", dayofweek(col(\"datee\"))) \\\n",
    "        .withColumn(\"quarter\", floor((month(col(\"datee\")) - 1) / 3) + 1) \\\n",
    "        .withColumn(\"day_name\", date_format(col(\"datee\"), \"EEEE\")) \\\n",
    "        .withColumn(\"month_name\", date_format(col(\"datee\"), \"MMMM\")) \\\n",
    "        .withColumn(\"is_weekend\", when(col(\"weekday\").isin([1, 7]), lit(1)).otherwise(lit(0)))\n",
    "\n",
    "    # Add surrogate key column\n",
    "    date_dim = date_dim.withColumn(\"date_sur_key\",concat(col('year'), col('month'), col('day')).cast('long'))\n",
    "\n",
    "    # Define the output directory for the date dimension\n",
    "    date_dim_path = \"hdfs:///data/retail_gold/date_dim\"\n",
    "\n",
    "    try:\n",
    "        # Write the date dimension to a single parquet file\n",
    "        date_dim.repartition(1) \\\n",
    "            .write.mode('overwrite') \\\n",
    "            .format('parquet') \\\n",
    "            .save(date_dim_path)\n",
    "        print(f\"Date dimension table saved to {date_dim_path}\")\n",
    "        write_df_to_table('retail_DWH', 'date_dim', date_dim, ['year'])\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "else:\n",
    "    date_dim = spark.read.parquet(date_dim_path)\n",
    "    print(\"Date dimension already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a80c64-bfbe-4672-9d23-5130596368ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:26:47.560712Z",
     "iopub.status.busy": "2024-07-12T22:26:47.560261Z",
     "iopub.status.idle": "2024-07-12T22:27:26.337033Z",
     "shell.execute_reply": "2024-07-12T22:27:26.337704Z"
    },
    "papermill": {
     "duration": 38.809095,
     "end_time": "2024-07-12T22:27:26.337974",
     "exception": false,
     "start_time": "2024-07-12T22:26:47.528879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 58308)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark2/python/pyspark/accumulators.py\", line 269, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark2/python/pyspark/serializers.py\", line 724, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in writing to branches_TRX_fact: An error occurred while calling o40.sql\n",
      "done\n"
     ]
    }
   ],
   "source": [
    " #fact One (offline)\n",
    "#print(input.columns)\n",
    "offline_fact=input_df.filter(col('is_online')==\"no\")\n",
    "columns_to_drop=['shipping_address','customer_fname','cusomter_lname','offer_1','offer_2',\n",
    "                'offer_3','offer_4','offer_5','product_name','product_category','customer_email']\n",
    "\n",
    "offline_fact=offline_fact.drop(*columns_to_drop)\n",
    "\n",
    "offline_fact=offline_fact.withColumn(\"transaction_date\", col(\"transaction_date\").cast(DateType()))\n",
    "final_price=(col('units') * col('unit_price') * (1 - col('discount_perc') / 100))\n",
    "offline_fact=offline_fact.withColumn(\"total_price\",final_price)\n",
    "offline_fact=offline_fact.join(updated_customers_dim, on='customer_id', how='left') \\\n",
    "                       .join(updated_product_dim, on='product_id', how='left') \\\n",
    "                       .join(date_dim, date_dim.datee == offline_fact.transaction_date, 'left') \\\n",
    "                       .join(updated_agent_dim, updated_agent_dim.sales_person_id == offline_fact.sales_agent_id, 'left') \\\n",
    "                       .join(updated_branches_dim, updated_branches_dim.branch_id == offline_fact.branch_id, 'left')\n",
    "\n",
    "offline_fact = offline_fact.withColumn(\"insertion_date\", date_format(lit(current_date()), \"yyyyMMdd\"))\n",
    "        \n",
    "offline_fact = offline_fact.select(\n",
    "    'transaction_id',\n",
    "    'branch_sur_key',\n",
    "    'product_sur_key',\n",
    "    'customer_sur_key',\n",
    "    'sales_agent_sur_key',\n",
    "    'date_sur_key',\n",
    "    'units',\n",
    "    'unit_price',\n",
    "    'discount_perc',\n",
    "    'total_price',\n",
    "    'payment_method',\n",
    "    'insertion_date'\n",
    ")\n",
    "\n",
    "fact_off_dim_path=\"hdfs://localhost:9000/data/retail_gold/offline_fact\"\n",
    "\n",
    "if check_if_exists(fact_off_dim_path):\n",
    "    #Hashing the records and compare the old and new dataframes\n",
    "    old_df = spark.read.parquet(fact_off_dim_path)\n",
    "    all_cols = F.concat_ws(\"\", *offline_fact.columns)\n",
    "    new_offline_fact = offline_fact.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    new_offline_fact = new_offline_fact.join(old_df, \"hashkey\", \"left_anti\")\n",
    "    if new_offline_fact.rdd.isEmpty() == False:\n",
    "        offline_fact=new_offline_fact.drop(\"hashkey\")\n",
    "        new_offline_fact=new_offline_fact.repartition(1)\n",
    "        #write the new data on HDFS\n",
    "        new_offline_fact.write.mode('append') \\\n",
    "                    .format('parquet') \\\n",
    "                    .save(fact_off_dim_path)\n",
    "        #inserting new data into hive branches_TRX_fact table\n",
    "        write_df_to_table('retail_DWH', 'branches_TRX_fact', offline_fact,['payment_method'])\n",
    "        print(\"done\")\n",
    "    else:\n",
    "        print(\"No new Data\")\n",
    "    \n",
    "else:\n",
    "    all_cols = F.concat_ws(\"\", *offline_fact.columns)\n",
    "    hashed_offline_fact = offline_fact.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    hashed_offline_fact=hashed_offline_fact.repartition(1)\n",
    "    #write the new data on HDFS\n",
    "    hashed_offline_fact.write.mode('overwrite') \\\n",
    "                .format('parquet') \\\n",
    "                .save(fact_off_dim_path)\n",
    "\n",
    "    #rename_in_hdfs(fact_off_dim_path,extension,name)\n",
    "    #inserting new data into hive branches_TRX_fact table\n",
    "    write_df_to_table('retail_DWH', 'branches_TRX_fact', offline_fact,['payment_method'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d55350",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0547af3-6762-40ca-b393-edfb56ef927f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:27:26.413105Z",
     "iopub.status.busy": "2024-07-12T22:27:26.408814Z",
     "iopub.status.idle": "2024-07-12T22:27:26.596162Z",
     "shell.execute_reply": "2024-07-12T22:27:26.594186Z"
    },
    "papermill": {
     "duration": 0.22573,
     "end_time": "2024-07-12T22:27:26.596338",
     "exception": true,
     "start_time": "2024-07-12T22:27:26.370608",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32869)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:32869)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-669d1dbe29ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# online_fact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0monline_fact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'is_online'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"yes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m columns_to_drop = ['customer_fname','cusomter_lname','sales_agent_id','offer_1','offer_2',\n\u001b[1;32m      5\u001b[0m                 'offer_3','offer_4','offer_5','product_name','product_category','customer_email']\n",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[1;32m   1650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    981\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \"\"\"\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    936\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:32869)"
     ]
    }
   ],
   "source": [
    "# online_fact\n",
    "online_fact = input_df.filter(col('is_online')==\"yes\")\n",
    "\n",
    "columns_to_drop = ['customer_fname','cusomter_lname','sales_agent_id','offer_1','offer_2',\n",
    "                'offer_3','offer_4','offer_5','product_name','product_category','customer_email']\n",
    "\n",
    "online_fact = online_fact.drop(*columns_to_drop)\n",
    "\n",
    "# cast transaction_date to date type \n",
    "online_fact=online_fact.withColumn(\"transaction_date\", col(\"transaction_date\").cast(DateType()))\n",
    "\n",
    "# calculate the final price \n",
    "final_price=(col('units') * col('unit_price') * (1 - (col('discount_perc') / 100)))\n",
    "online_fact=online_fact.withColumn(\"total_price\",final_price)\n",
    "\n",
    "#processing address column\n",
    "split_address_col=split(col(\"shipping_address\"),'/')\n",
    "online_fact=online_fact.withColumn('street',split_address_col.getItem(0))\\\n",
    "                        .withColumn('city',split_address_col.getItem(1))\\\n",
    "                        .withColumn('state',split_address_col.getItem(2))\\\n",
    "                        .withColumn('postal_code',split_address_col.getItem(3))\n",
    "\n",
    "# Join with dimension tables using left join\n",
    "online_fact = online_fact.join(updated_customers_dim, on='customer_id', how='left') \\\n",
    "                         .join(updated_product_dim, on='product_id', how='left') \\\n",
    "                         .join(date_dim, date_dim.datee == online_fact.transaction_date, 'left')\n",
    "online_fact = online_fact.withColumn('insertion_date', date_format(lit(current_date()), \"yyyyMMdd\"))\n",
    "\n",
    "\n",
    "online_fact = online_fact.select(\n",
    "    'transaction_id',\n",
    "    'units',\n",
    "    'unit_price',\n",
    "    'payment_method',\n",
    "    'discount_perc',\n",
    "    'total_price',\n",
    "    'customer_sur_key',\n",
    "    'product_sur_key',\n",
    "    'date_sur_key',\n",
    "    'street',\n",
    "    'city',\n",
    "    'state',\n",
    "    'postal_code',\n",
    "    'insertion_date'\n",
    ")\n",
    "\n",
    "online_fact_path=\"hdfs://localhost:9000/data/retail_gold/online_fact\"\n",
    "\n",
    "if check_if_exists(online_fact_path):\n",
    "    #Hashing the records and compare the old and new dataframes\n",
    "    old_df = spark.read.parquet(online_fact_path)\n",
    "    all_cols = F.concat_ws(\"\", *online_fact.columns)\n",
    "    new_online_fact = online_fact.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    new_online_fact = new_online_fact.join(old_df, \"hashkey\", \"left_anti\")\n",
    "    if new_online_fact.rdd.isEmpty() == False:\n",
    "        online_fact=new_online_fact.drop(\"hashkey\")\n",
    "        new_online_fact=new_online_fact.repartition(1)\n",
    "        #writing the new data on HDFS\n",
    "        new_online_fact.write.mode('append') \\\n",
    "                    .format('parquet') \\\n",
    "                    .save(online_fact_path)\n",
    "        #inserting new data into hive online_TRX_fact table\n",
    "        write_df_to_table('retail_DWH', 'online_TRX_fact', online_fact,['payment_method'])\n",
    "        print(\"done\")\n",
    "    else:\n",
    "        print(\"No new Data\")    \n",
    "else:\n",
    "    all_cols = F.concat_ws(\"\", *online_fact.columns)\n",
    "    hashed_online_fact = online_fact.withColumn(\"hashkey\", F.md5(all_cols))\n",
    "    hashed_online_fact=hashed_online_fact.repartition(1)\n",
    "    #writing the new data on HDFS\n",
    "    hashed_online_fact.write.mode('overwrite') \\\n",
    "             .format('parquet') \\\n",
    "             .save(online_fact_path)\n",
    "    #inserting new data into hive online_TRX_fact table\n",
    "    write_df_to_table('retail_DWH', 'online_TRX_fact', online_fact,['payment_method'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d542a-c5a7-4a02-832a-ba3b2dd8b490",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Daily report for the B2B team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586e4eb-ded7-48f3-b4c1-7e6686d10bef",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if hour_str == 23:\n",
    "    offline_fact = spark.read.parquet(\"hdfs:///data/retail_gold/offline_fact/\")\n",
    "    daily_fact = offline_fact.filter(col('insertion_date') == date_str)\n",
    "    daily_fact = daily_fact.join(updated_agent_dim, on ='sales_agent_sur_key' , how='left') \\\n",
    "                            .join(updated_product_dim, on='product_sur_key', how='left')\n",
    "    daily_fact = daily_fact.select('name', 'product_name', 'units')\n",
    "    daily_report = daily_fact.groupBy(\"name\",\"product_name\").agg({\"units\": \"sum\"})\n",
    "    daily_report.show()\n",
    "    daily_report = daily_report.coalesce(1)\n",
    "    daily_report.write.option(\"header\", \"true\").csv(f\"file:///home/itversity/itversity-material/Retail_pipeline_project/Daily_report/report_{date_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d4673-f608-4d17-8235-0ca887a397f8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a4b08-685a-48e2-80fc-886119f58657",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#updated_customers_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a8dce-4533-40d3-8ed1-4f5a992eb308",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#updated_product_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062bea90-867f-4858-868b-ee411fa648f7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#date_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6c1d9-1d96-42b1-96ee-3bf39c991af9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#online_fact.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ace1a-d742-4f75-bac2-95a406876b93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#offline_fact.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a697b5-e4a6-4061-bea7-2fee448e60d1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#updated_agent_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae9ace6-3f49-4d59-8a82-f22ffce7db57",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#updated_branches_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e964d5-08c9-49d8-8853-a5cdc2941f08",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cust = spark.read.parquet(\"hdfs:///data/retail_gold/customer_dim/\")\n",
    "#cust.show()\n",
    "#cust.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a25340-9ea5-403d-a2f7-2cd58aa843c3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"DESCRIBE FORMATTED retail_DWH.branches_TRX_fact\").show(50,truncate=False)\n",
    "\n",
    "#SET hive.exec.dynamic.partition = true;\n",
    "#SET hive.exec.dynamic.partition.mode = nonstrict;\n",
    "#SET hive.mapred.mode = nonstrict;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 79.530851,
   "end_time": "2024-07-12T22:27:29.910891",
   "environment_variables": {},
   "exception": true,
   "input_path": "/home/itversity/itversity-material/Retail_pipeline_project/Gold_layer_transformation.ipynb",
   "output_path": "/home/itversity/itversity-material/Retail_pipeline_project/papermill_logs/Gold_output.json",
   "parameters": {},
   "start_time": "2024-07-12T22:26:10.380040",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}