{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a0a705e-d013-4349-b873-78fcb1d1cda5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'isEmpty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-03b4ed7c0e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregexp_replace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhen\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmonotonically_increasing_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayofmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweekofyear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayofweek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdense_rank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msubstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0misEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPickleSerializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoBatchedSerializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'isEmpty'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, regexp_replace, trim,when ,monotonically_increasing_id,lit,year, month, dayofmonth, weekofyear, dayofweek, date_format,floor,dense_rank,\\\n",
    "substring,concat,split\n",
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
    "from datetime import date, datetime, timedelta\n",
    "import subprocess\n",
    "from py4j.java_gateway import java_import\n",
    "import os\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbd3353-908f-4d90-850f-c36424d80817",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .appName(\"Gold_layer_transformations\")\\\n",
    "    .config(\"spark.eventLog.logBlockUpdates.enabled\", True)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6120cae5-dc75-4881-b2c2-139445c22d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "date_str = now.strftime(\"%Y%m%d\")\n",
    "hour_str = now.strftime(\"%H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf8fd60-448d-4761-9e0c-c69f3851b8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-----------+--------------+--------------+--------------+---------+----------+------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+----+------+--------------------+-------------+\n",
      "|transaction_date|  transaction_id|customer_id|customer_fname|cusomter_lname|sales_agent_id|branch_id|product_id|product_name|product_category|offer_1|offer_2|offer_3|offer_4|offer_5|units|unit_price|is_online|payment_method|shipping_address|logs|source|      customer_email|discount_perc|\n",
      "+----------------+----------------+-----------+--------------+--------------+--------------+---------+----------+------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+----+------+--------------------+-------------+\n",
      "|       2022-7-19|trx-878108770002|      85513|     Alexander|       Johnson|            10|        6|        27|        Iron|      Appliances|   null|   null|   null|   null|   null|    7|     29.99|       no|          Cash|            null|null|  null|alexander.johnson...|            0|\n",
      "|        2023-8-6|trx-349443438637|      85510|           Ava|         Smith|             2|        6|        28|  Hair Dryer|      Appliances|   null|   True|   null|   null|   null|   10|     19.99|       no|          Cash|            null|null|  null| ava.smith@gmail.com|           10|\n",
      "|      2022-12-28|trx-045891300294|      85553|           Mia|         Brown|             6|        4|        13|     Printer|     Electronics|   null|   null|   True|   null|   null|    2|    149.99|       no|   Credit Card|            null|null|  null|mia.brown@hotmail...|           15|\n",
      "|       2023-6-28|trx-756996252944|      85520|        Olivia|        Taylor|             3|        2|        12|     Monitor|     Electronics|   null|   null|   null|   null|   null|    6|    299.99|       no|   Credit Card|            null|null|  null|olivia.taylor@out...|            0|\n",
      "|        2023-9-5|trx-491216466700|      85539|          John|         Moore|             8|        3|         5|     T-Shirt|        Clothing|   True|   null|   null|   null|   null|    1|     19.99|       no|   Credit Card|            null|null|  null|john.moore@gmail.com|            5|\n",
      "+----------------+----------------+-----------+--------------+--------------+--------------+---------+----------+------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+----+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_trans = spark.read.csv(f\"hdfs:///data/retail_silver/20240706/21/sales_transactions_SS_cleaned_20240706_21.csv\", header='true')\n",
    "#input = spark.read.csv(f\"hdfs:///data/retail_silver/{date_str}/12/sales_transactions_SS_cleaned_{date_str}_12.csv\", header='true')\n",
    "input_trans.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b82a6dd-65c8-4f50-b918-5e44ca9788f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_date', 'transaction_id', 'customer_id', 'customer_fname', 'cusomter_lname', 'sales_agent_id', 'branch_id', 'product_id', 'product_name', 'product_category', 'offer_1', 'offer_2', 'offer_3', 'offer_4', 'offer_5', 'units', 'unit_price', 'is_online', 'payment_method', 'shipping_address', 'logs', 'source', 'customer_email', 'discount_perc']\n"
     ]
    }
   ],
   "source": [
    "print(input_trans.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d197f3-24a9-47f2-9bc5-566b12afe3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function_to_rename_in_hdfs\n",
    "def rename_in_hdfs(golden_layer_path,file_extension,name):\n",
    "    # Run the Hadoop fs -ls command to list files\n",
    "    list_files_process = subprocess.run([\"hadoop\", \"fs\", \"-ls\", golden_layer_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Check for errors\n",
    "    if list_files_process.returncode != 0:\n",
    "        print(f\"Error listing files in {golden_layer_path}: {list_files_process.stderr.decode()}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Decode stdout to string format and split lines\n",
    "    stdout_str = list_files_process.stdout.decode()\n",
    "    file_list = stdout_str.splitlines()\n",
    "\n",
    "    # Find the file to rename based on criteria\n",
    "    file_to_rename = None\n",
    "    for line in file_list:\n",
    "        if line.endswith(file_extension):\n",
    "            file_to_rename = line.split()[-1].strip()\n",
    "            break\n",
    "\n",
    "    # Check if a file matching the criteria was found\n",
    "    if file_to_rename:\n",
    "        #new_directory = f\"{golden_layer_path}/{name}\"\n",
    "        new_filename = f\"{golden_layer_path}/{name}{file_extension}\"\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        #subprocess.run([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", new_directory])\n",
    "\n",
    "        # Move (rename) the file\n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-mv\", file_to_rename, new_filename])\n",
    "\n",
    "        print(f\"File moved and renamed to: {new_filename}\")\n",
    "    else:\n",
    "        print(\"File matching the criteria not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ed0bd5d-c4c2-444c-9648-d18e563e89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_exists(path):\n",
    "    jvm = spark._jvm\n",
    "    jsc = spark._jsc\n",
    "    fs = jvm.org.apache.hadoop.fs.FileSystem.get(jsc.hadoopConfiguration())\n",
    "    return fs.exists(jvm.org.apache.hadoop.fs.Path(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bba9bfe8-67ad-4dcc-afdd-79f262fac181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File moved and renamed to: hdfs:///data/golden_layer/cust_dim/cust_dim.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(customer_sur_key=42949672960, customer_id='85485', customer_fname='Emma', cusomter_lname='Williams', customer_email='emma.williams@hotmail.com'),\n",
       " Row(customer_sur_key=51539607552, customer_id='85529', customer_fname='Mia', cusomter_lname='Williams', customer_email='mia.williams@yahoo.com'),\n",
       " Row(customer_sur_key=68719476736, customer_id='85509', customer_fname='Emma', cusomter_lname='Williams', customer_email='emma.williams@gmail.com'),\n",
       " Row(customer_sur_key=85899345920, customer_id='85547', customer_fname='Olivia', cusomter_lname='Smith', customer_email='olivia.smith@outlook.com'),\n",
       " Row(customer_sur_key=120259084288, customer_id='85476', customer_fname='Alexander', cusomter_lname='Brown', customer_email='alexander.brown@outlook.com')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#write customer dim in HDFS\n",
    "\n",
    "cust_dim = input_trans.dropDuplicates(['customer_id'])\n",
    "# Add monotonically increasing id column\n",
    "cust_dim = cust_dim.withColumn('customer_sur_key', monotonically_increasing_id())\n",
    "\n",
    "#to write cust_dim in one file \n",
    "cust_dim=cust_dim.repartition(1)\n",
    "golden_layer_path=\"hdfs:///data/golden_layer/cust_dim\"\n",
    "file_extension = \".csv\"\n",
    "name='cust_dim'\n",
    "\n",
    "#make customer dim \n",
    "cust_dim=cust_dim.select('customer_sur_key','customer_id', 'customer_fname', 'cusomter_lname', 'customer_email') \n",
    "cust_dim.write.mode('overwrite') \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .format('csv') \\\n",
    "        .save(golden_layer_path)\n",
    "\n",
    "# Define your directory path and file criteria\n",
    "#directory_path = customer_dim_path\n",
    "#golden_layer_path=\"hdfs:///data/golden_layer\"\n",
    "file_extension = \".csv\"\n",
    "name='cust_dim'\n",
    "rename_in_hdfs(golden_layer_path,file_extension,name)\n",
    "cust_dim.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff04be8c-f132-4f0a-99f4-0474f7806761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File moved and renamed to: hdfs:///data/golden_layer/prodcut_dim/product_dim.csv\n"
     ]
    }
   ],
   "source": [
    "#product_dim\n",
    "product_dim=input_trans.dropDuplicates(['product_id'])\n",
    "product_dim = product_dim.withColumn('product_sur_key', monotonically_increasing_id())\n",
    "#to write product_dim in one file \n",
    "product_dim=product_dim.repartition(1)\n",
    "\n",
    "\n",
    "golden_layer_path=\"hdfs:///data/golden_layer/prodcut_dim\"\n",
    "file_extension = \".csv\"\n",
    "name='product_dim'\n",
    "\n",
    "#make customer dim \n",
    "product_dim=product_dim.select('product_sur_key','product_id', 'product_name', 'product_category') \n",
    "product_dim.write.mode('overwrite') \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .format('csv') \\\n",
    "            .save(golden_layer_path)\n",
    "\n",
    "rename_in_hdfs(golden_layer_path,file_extension,name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f04e839-8863-448a-90e5-13e1ff09c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------+-----+-------------+\n",
      "|branch_id|   location|establish_date|class|surrogate_key|\n",
      "+---------+-----------+--------------+-----+-------------+\n",
      "|        1|   New York|    2017-01-15|    A|            0|\n",
      "|        2|Los Angeles|    2016-07-28|    B|            1|\n",
      "|        3|    Chicago|    2015-03-10|    A|            2|\n",
      "|        4|    Houston|    2016-11-05|    D|            3|\n",
      "|        5|    Phoenix|    2017-09-20|    C|            4|\n",
      "|        6|   Oklahoma|    2016-09-20|    A|            5|\n",
      "+---------+-----------+--------------+-----+-------------+\n",
      "\n",
      "File moved and renamed to: hdfs:///data/golden_layer/branches_dim/branches_dim.csv\n"
     ]
    }
   ],
   "source": [
    "#branches Dim\n",
    "# Define the file path for the initial CSV data and the golden layer path on HDFS\n",
    "file_path = f\"hdfs:///data/retail_bronze/{date_str}/{hour_str}/branches_SS_raw_{date_str}__{date_str}_{hour_str}.csv\"\n",
    "golden_layer_path = \"hdfs:///data/golden_layer/branches_dim\"\n",
    "manual_path=f\"hdfs:///data/retail_bronze/20240706/12/branches_SS_raw_20240706_12.csv\"\n",
    "file_extension = \".csv\"\n",
    "name=\"branches_dim\"\n",
    "# Create the \"golden layer\" directory on HDFS if it doesn't exist\n",
    "subprocess.run([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", golden_layer_path])\n",
    "\n",
    "# Load the CSV data into a PySpark DataFrame\n",
    "branches_dim = spark.read.option(\"header\", \"true\").csv(manual_path)\n",
    "\n",
    "# Convert establish_date to date type if needed\n",
    "branches_dim = branches_dim.withColumn(\"establish_date\", col(\"establish_date\").cast(\"date\"))\n",
    "\n",
    "# Add a surrogate key column\n",
    "branches_dim = branches_dim.withColumn(\"surrogate_key\", monotonically_increasing_id())\n",
    "\n",
    "# Show the DataFrame with the surrogate key\n",
    "branches_dim.show()\n",
    "\n",
    "# Save the DataFrame to the golden layer folder on HDFS in CSV format\n",
    "branches_dim.write.option(\"header\", \"true\").mode(\"overwrite\").csv(golden_layer_path)\n",
    "rename_in_hdfs(golden_layer_path,file_extension,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c9bafc-3338-4fd9-bc8f-1797874a62bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+----------+-------------+\n",
      "|sales_person_id|              name| hire_date|surrogate_key|\n",
      "+---------------+------------------+----------+-------------+\n",
      "|              1|          John Doe|2020-06-10|            0|\n",
      "|              2|        Jane Smith|2021-06-08|            1|\n",
      "|              3|   Michael Johnson|2019-07-22|            2|\n",
      "|              4|       Emily Brown|2018-11-12|            3|\n",
      "|              5|      David Wilson|2020-06-23|            4|\n",
      "|              6|       Emma Taylor|2018-08-09|            5|\n",
      "|              7|Christopher Miller|2018-07-05|            6|\n",
      "|              8|      Olivia Davis|2019-12-08|            7|\n",
      "|              9|   Daniel Martinez|2019-07-19|            8|\n",
      "|             10|      Sophia Moore|2019-11-10|            9|\n",
      "|             11|         john wick|2018-07-10|           10|\n",
      "+---------------+------------------+----------+-------------+\n",
      "\n",
      "File moved and renamed to: hdfs:///data/golden_layer/agent_DIM/agent.csv\n"
     ]
    }
   ],
   "source": [
    "#agent dim\n",
    "# Define the file path for the initial CSV data and the golden layer path on HDFS\n",
    "file_path = f\"hdfs:///data/retail_bronze/{date_str}/{hour_str}/sales_agents_SS_raw_{date_str}__{date_str}_{hour_str}.csv\"\n",
    "golden_layer_path = \"hdfs:///data/golden_layer/agent_DIM\"\n",
    "manual_path=\"/data/retail_bronze/20240706/12/sales_agents_SS_raw_20240706_12.csv\"\n",
    "file_extension = \".csv\"\n",
    "name=\"agent\"\n",
    "# Create the \"golden layer\" directory on HDFS if it doesn't exist\n",
    "subprocess.run([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", golden_layer_path])\n",
    "\n",
    "# Load the CSV data into a PySpark DataFrame\n",
    "agent_dim = spark.read.option(\"header\", \"true\").csv(manual_path)\n",
    "\n",
    "# Convert establish_date to date type if needed\n",
    "agent_dim = agent_dim.withColumn(\"hire_date\", col(\"hire_date\").cast(\"date\"))\n",
    "\n",
    "# Add a surrogate key column\n",
    "agent_dim = agent_dim.withColumn(\"surrogate_key\", monotonically_increasing_id())\n",
    "agent_dim.write.option(\"header\", \"true\").mode(\"overwrite\").csv(golden_layer_path)\n",
    "\n",
    "\n",
    "# Show the DataFrame with the surrogate key\n",
    "agent_dim.show()\n",
    "\n",
    "rename_in_hdfs(golden_layer_path,file_extension,name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ce9bacf-bbee-45a8-a226-5ec4d5f96fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date dimension table saved to hdfs:///data/golden_layer/date_dim\n",
      "File moved and renamed to: hdfs:///data/golden_layer/date_dim/date_dim.csv\n"
     ]
    }
   ],
   "source": [
    "#create date dimension\n",
    "# Generate date range\n",
    "\n",
    "start_date = date(2022, 1, 1)\n",
    "end_date = date(2024, 12, 31)\n",
    "\n",
    "date_range = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "date_df = spark.createDataFrame([(d,) for d in date_range], [\"date\"]).withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# Add date attributes\n",
    "date_dim = date_df.withColumn(\"year\", year(col(\"date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"date\"))) \\\n",
    "    .withColumn(\"week\", weekofyear(col(\"date\"))) \\\n",
    "    .withColumn(\"weekday\", dayofweek(col(\"date\"))) \\\n",
    "    .withColumn(\"quarter\", floor((month(col(\"date\")) - 1) / 3) + 1) \\\n",
    "    .withColumn(\"day_name\", date_format(col(\"date\"), \"EEEE\")) \\\n",
    "    .withColumn(\"month_name\", date_format(col(\"date\"), \"MMMM\")) \\\n",
    "    .withColumn(\"is_weekend\", when(col(\"weekday\").isin([1, 7]), lit(1)).otherwise(lit(0)))\n",
    "\n",
    "# Add surrogate key column\n",
    "date_dim = date_dim.withColumn(\"date_sur_key\", concat(col('day'),col('month'),col('year')))\n",
    "\n",
    "\n",
    "# Define the output directory for the date dimension\n",
    "date_dim_path = \"hdfs:///data/golden_layer/date_dim\"\n",
    "\n",
    "try:\n",
    "    # Write the date dimension to a single CSV file\n",
    "    date_dim.repartition(1) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .format('csv') \\\n",
    "        .save(date_dim_path)\n",
    "    print(f\"Date dimension table saved to {date_dim_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    \n",
    "#to rename csv file in date dim\n",
    "#date_dim_path=\"hdfs:///data/golden_layer\"\n",
    "file_extension = \".csv\"\n",
    "name=\"date_dim\"\n",
    "rename_in_hdfs(date_dim_path,file_extension,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a80c64-bfbe-4672-9d23-5130596368ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to create fact table \n",
    "#fact One (offline)\n",
    "#print(input.columns)\n",
    "offline_fact=input_trans.filter(col('is_online')==\"no\")\n",
    "columns_to_drop=['shipping_address','customer_fname','cusomter_lname','sales_agent_id','offer_1','offer_2',\n",
    "                'offer_3','offer_4','offer_5','product_name','product_category','customer_email']\n",
    "offline_fact=offline_fact.drop(*columns_to_drop)\n",
    "#print(offline_fact.columns)\n",
    "#print(offline_fact.take(5))\n",
    "#print(cust_dim.columns)\n",
    "offline_fact=offline_fact.withColumn(\"transaction_date\", col(\"transaction_date\").cast(DateType()))\n",
    "final_price=(col('units') * col('unit_price') * (1 - col('discount_perc') / 100))\n",
    "offline_fact=offline_fact.withColumn(\"total_price\",final_price)\n",
    "offline_fact=cust_dim.join(offline_fact,on='customer_id',how='left').join(product_dim,on='product_id',how='right').\\\n",
    "                    join(date_dim,date_dim.date == offline_fact.transaction_date,'left')\n",
    "#offline_fact=product_dim.join(offline_fact,on='product_id',how='left')\n",
    "offline_fact\n",
    "offline_fact = offline_fact.select(\n",
    "    'transaction_id',\n",
    "    'units',\n",
    "    'payment_method',\n",
    "    'discount_perc',\n",
    "    'total_price',\n",
    "    'transaction_date',     \n",
    "    'customer_sur_key',\n",
    "    'product_sur_key',\n",
    "    'date_sur_key'\n",
    ")\n",
    "\n",
    "fact_off_dim_path=\"hdfs:///data/golden_layer/offline_fact\"\n",
    "name =\"offline_fact\"\n",
    "extension = \".csv\"\n",
    "full_file_path = f\"{fact_off_dim_path}/{name}{extension}\"\n",
    "if check_if_exists(full_file_path):\n",
    "    old_df = spark.read.csv(full_file_path, header = 'true')\n",
    "    unioned_offline_fact_df = old_df.union(offline_fact)\n",
    "    new_offline_fact = unioned_offline_fact_df.dropDuplicates(['transaction_id'])\n",
    "    if new_offline_fact.rdd.isEmpty() == False:\n",
    "        new_offline_fact=new_offline_fact.repartition(1)\n",
    "        new_offline_fact.write.mode('overwrite') \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .format('csv') \\\n",
    "                    .save(f\"{fact_off_dim_path}/tmp\")\n",
    "        rename_in_hdfs(f\"{fact_off_dim_path}/tmp\",extension,name) \n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-rm\", full_file_path])\n",
    "        subprocess.run([\"hadoop\", \"fs\", \"-mv\", f\"{fact_off_dim_path}/tmp/{name}{extension}\" ,fact_off_dim_path])\n",
    "        print(\"done\")\n",
    "        \n",
    "    \n",
    "else:\n",
    "    offline_fact=offline_fact.repartition(1)\n",
    "    offline_fact.write.mode('overwrite') \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .format('csv') \\\n",
    "                .save(fact_off_dim_path)\n",
    "\n",
    "    rename_in_hdfs(fact_off_dim_path,extension,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0547af3-6762-40ca-b393-edfb56ef927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'units', 'payment_method', 'discount_perc', 'total_price', 'transaction_date', 'customer_sur_key', 'product_sur_key', 'date_sur_key', 'street', 'city', 'state', 'postal_code']\n",
      "False\n",
      "File matching the criteria not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(transaction_id='trx-273956652971', units='8', payment_method='Stripe', discount_perc='15', total_price=407.932, transaction_date=datetime.date(2022, 3, 28), customer_sur_key=1348619730944, product_sur_key=25769803776, date_sur_key='2832022', street='12 Netherclift Way', city='Savannah', state='GA', postal_code='31411'),\n",
       " Row(transaction_id='trx-916643027495', units='6', payment_method='PayPal', discount_perc='5', total_price=170.94299999999998, transaction_date=datetime.date(2022, 3, 28), customer_sur_key=1168231104512, product_sur_key=60129542144, date_sur_key='2832022', street='40 Strawberry Lane', city='Manchester', state='CT', postal_code='06040'),\n",
       " Row(transaction_id='trx-211274182982', units='7', payment_method='Stripe', discount_perc='5', total_price=199.43349999999998, transaction_date=datetime.date(2022, 3, 28), customer_sur_key=1133871366144, product_sur_key=60129542144, date_sur_key='2832022', street='5921 Ashwood Bluff Drive', city='Louisville', state='KY', postal_code='40207'),\n",
       " Row(transaction_id='trx-069314636581', units='3', payment_method='Credit Card', discount_perc='0', total_price=89.97, transaction_date=datetime.date(2022, 3, 28), customer_sur_key=652835028992, product_sur_key=60129542144, date_sur_key='2832022', street='210 Depot Street', city='Woodford', state='VT', postal_code='05201'),\n",
       " Row(transaction_id='trx-525404625659', units='8', payment_method='Stripe', discount_perc='0', total_price=7199.92, transaction_date=datetime.date(2022, 3, 28), customer_sur_key=670014898176, product_sur_key=94489280512, date_sur_key='2832022', street='5370 Business Park Drive', city='Montgomery', state='AL', postal_code='36116')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#online_fact\n",
    "online_fact=input_trans.filter(col('is_online')==\"yes\")\n",
    "columns_to_drop=['customer_fname','cusomter_lname','sales_agent_id','offer_1','offer_2',\n",
    "                'offer_3','offer_4','offer_5','product_name','product_category','customer_email']\n",
    "online_fact=online_fact.drop(*columns_to_drop)\n",
    "#print(online_fact.columns)\n",
    "#cast transaction_date to date type \n",
    "online_fact=online_fact.withColumn(\"transaction_date\", col(\"transaction_date\").cast(DateType()))\n",
    "#calculate the final price \n",
    "final_price=(col('units') * col('unit_price') * (1 - col('discount_perc') / 100))\n",
    "online_fact=online_fact.withColumn(\"total_price\",final_price)\n",
    "#processing address column\n",
    "split_address_col=split(col(\"shipping_address\"),'/')\n",
    "online_fact=online_fact.withColumn('street',split_address_col.getItem(0))\\\n",
    "                        .withColumn('city',split_address_col.getItem(1))\\\n",
    "                        .withColumn('state',split_address_col.getItem(2))\\\n",
    "                        .withColumn('postal_code',split_address_col.getItem(3))\n",
    "online_fact\n",
    "#join dataframe togther \n",
    "online_fact=cust_dim.join(online_fact,on='customer_id',how='left').join(product_dim,on='product_id',how='right').\\\n",
    "                    join(date_dim,date_dim.date == online_fact.transaction_date,'left')\n",
    "online_fact = online_fact.select(\n",
    "    'transaction_id',\n",
    "    'units',\n",
    "    'payment_method',\n",
    "    'discount_perc',\n",
    "    'total_price',\n",
    "    'transaction_date',     \n",
    "    'customer_sur_key',\n",
    "    'product_sur_key',\n",
    "    'date_sur_key',\n",
    "    'street',\n",
    "    'city',\n",
    "    'state',\n",
    "    'postal_code'\n",
    ")\n",
    "print(online_fact.columns)\n",
    "online_fact_path=\"hdfs:///data/golden_layer/online_fact\"\n",
    "name=\"online_fact\"\n",
    "extension = \".csv\"\n",
    "print(check_if_exists(f\"{online_fact}/{name}/{extension}\"))\n",
    "online_fact=online_fact.repartition(1)\n",
    "online_fact.write.mode('overwrite') \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .format('parquet') \\\n",
    "            .save(online_fact_path)\n",
    "\n",
    "rename_in_hdfs(online_fact_path,extension,name)\n",
    "online_fact.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2eeb4-84c1-43e9-b691-5aecf3115440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
