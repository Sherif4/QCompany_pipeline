{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61f5df6-93de-446c-ba53-6798b39b3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pydoop.hdfs as hdfs\n",
    "import re\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c944c3-3a79-48a9-a830-150047377f43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['group1', 'group2', 'group3', 'group4', 'group5', 'group6']\n"
     ]
    }
   ],
   "source": [
    "dirs = os.listdir('/home/itversity/itversity-material/Retail_pipeline_project/data')\n",
    "groups = [d for d in dirs if os.path.isdir(os.path.join('/home/itversity/itversity-material/Retail_pipeline_project/data', d)) and d.startswith('group')]\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b3f40-a585-4cb9-97eb-dce24adde8c3",
   "metadata": {},
   "source": [
    "- Function to write ingested files into a text file to skip them on the next round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08151d1d-1e47-48eb-bc42-56a3ffe433d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_dirs_to_file(file_path, group):\n",
    "    with open(file_path, 'r') as file:\n",
    "            existing_content = file.read().splitlines()\n",
    "            \n",
    "    with open(file_path, 'a') as file:\n",
    "        if group not in existing_content:\n",
    "            file.write(f\"{group}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42751f3-be19-4799-9ec2-480b0f943eea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/home/itversity/itversity-material/Retail_pipeline_project/data/written_groups.txt', 'r') as file:\n",
    "    skip_dir = file.read().splitlines()\n",
    "    \n",
    "filtered_dir = [group for group in groups if group not in skip_dir]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aad0ad-74eb-41e4-a13f-7aa51b49fa7d",
   "metadata": {
    "tags": []
   },
   "source": [
    " Function to rename the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee161aca-a9a7-47a1-8044-49354633b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files_and_upload(local_directory, hdfs_directory):\n",
    "    files = os.listdir(local_directory)\n",
    "    print(files)\n",
    "    # Rename files\n",
    "    for file_name in files:\n",
    "        old_file_path = os.path.join(local_directory, file_name)\n",
    "        \n",
    "        # Get the current date and hour\n",
    "        now = datetime.now()\n",
    "        date_str = now.strftime(\"%Y%m%d\")\n",
    "        hour_str = now.strftime(\"%H\")\n",
    "        \n",
    "        # Split the file name and extension\n",
    "        split_name= os.path.splitext(file_name)\n",
    "        # Create the new file name with the extension at the end\n",
    "        print(split_name)\n",
    "        new_parquet_file_name = f\"{split_name[0][:-2]}_{date_str}_{hour_str}.parquet\"\n",
    "        new_parquet_file_path = os.path.join(local_directory, new_parquet_file_name)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(old_file_path, header = 0)\n",
    "            prq = df.to_parquet(new_parquet_file_path)\n",
    "        except:\n",
    "            print(\"files already converted and ranamed\")\n",
    "            \n",
    "        # Upload the renamed file to HDFS\n",
    "        hdfs.mkdir(f\"/data/retail_bronze/{date_str}/{hour_str}\")\n",
    "        day_directory = os.path.join(hdfs_directory, date_str)\n",
    "        hour_directory = os.path.join(day_directory, hour_str)\n",
    "        hdfs_file_path = os.path.join(hour_directory, new_parquet_file_name)\n",
    "        hdfs.put(new_parquet_file_path, hdfs_file_path)\n",
    "        print(f\"Uploaded {new_parquet_file_name} to {hdfs_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264e4db8-a75b-4e6e-b6d1-50a427a03393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['branches_SS_raw_1.csv', 'sales_agents_SS_raw_1.csv', 'sales_transactions_SS_raw_1.csv']\n",
      "('branches_SS_raw_1', '.csv')\n",
      "Uploaded branches_SS_raw_20240713_07.parquet to /data/retail_bronze/20240713/07/branches_SS_raw_20240713_07.parquet\n",
      "('sales_agents_SS_raw_1', '.csv')\n",
      "Uploaded sales_agents_SS_raw_20240713_07.parquet to /data/retail_bronze/20240713/07/sales_agents_SS_raw_20240713_07.parquet\n",
      "('sales_transactions_SS_raw_1', '.csv')\n",
      "Uploaded sales_transactions_SS_raw_20240713_07.parquet to /data/retail_bronze/20240713/07/sales_transactions_SS_raw_20240713_07.parquet\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if filtered_dir:\n",
    "        rename_files_and_upload(f\"/home/itversity/itversity-material/Retail_pipeline_project/data/{filtered_dir[0]}\", '/data/retail_bronze')\n",
    "    else:\n",
    "        raise SystemExit(f\"No new groups found\")  # Exit with code 1 for missing path\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise SystemExit(1)  # Exit with code 1 for other errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d53ce-08d8-4853-8c98-c62a95ab7a32",
   "metadata": {},
   "source": [
    "write the already ingested files to the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e0e8bb8-5b4e-4f5c-8203-f89e6239e151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "write_dirs_to_file('/home/itversity/itversity-material/Retail_pipeline_project/data/written_groups.txt', filtered_dir[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d574ef1-26d6-47d2-a827-90b2f2254f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360026c-820e-4bfc-b7cf-3ea130055f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
